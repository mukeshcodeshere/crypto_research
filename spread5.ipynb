{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb999ec",
   "metadata": {},
   "source": [
    "## This code tests for stationarity & correlation in spreads / spreads-of-spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54e099",
   "metadata": {},
   "source": [
    "# Load data - polars only + vectorized operations to speed up everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6811460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading symbols: 100%|██████████| 4/4 [00:00<00:00,  8.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, coint\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration parameters\n",
    "SYMBOLS = ['BTCUSD', 'ETHUSD', 'SOLUSD', 'XRPUSD']\n",
    "DATA_DIR = Path(\"coinbase/5m\")\n",
    "SIGNIFICANCE_LEVEL = 0.05\n",
    "MIN_OBSERVATIONS = 100\n",
    "\n",
    "columns = [\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "def drop_nan_rows(df):\n",
    "    mask = ~(\n",
    "        df[\"open\"].is_nan() |\n",
    "        df[\"high\"].is_nan() |\n",
    "        df[\"low\"].is_nan() |\n",
    "        df[\"close\"].is_nan() |\n",
    "        df[\"volume\"].is_nan()\n",
    "    )\n",
    "    return df.filter(mask)\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "for symbol in tqdm(SYMBOLS, desc=\"Loading symbols\"):\n",
    "    symbol_dfs = []\n",
    "    for year_dir in DATA_DIR.iterdir():\n",
    "        if year_dir.is_dir():\n",
    "            file_path = year_dir / f\"{symbol}_5m_{year_dir.name}.csv\"\n",
    "            if file_path.exists():\n",
    "                df = pl.read_csv(file_path, has_header=False, new_columns=columns)\n",
    "                df = drop_nan_rows(df)\n",
    "                symbol_dfs.append(df)\n",
    "\n",
    "    if symbol_dfs:\n",
    "        combined_df = pl.concat(symbol_dfs)\n",
    "        combined_df = combined_df.with_columns(\n",
    "            (pl.col(\"timestamp\") * 1000).cast(pl.Datetime(\"ms\")).alias(\"timestamp\")\n",
    "        )\n",
    "        dataframes[symbol] = combined_df\n",
    "    else:\n",
    "        print(f\"No files found for {symbol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c89fa3",
   "metadata": {},
   "source": [
    "## Make spread & spread-of-spread dataframes [ ratio of close ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f2b4483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing spreads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing spreads: 100%|██████████| 6/6 [00:00<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing spread-of-spreads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing spread-of-spreads: 100%|██████████| 15/15 [00:00<00:00, 106.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def compute_spread_df(df1, df2, name1, name2):\n",
    "    joined = df1.join(df2, on=\"timestamp\", how=\"inner\", suffix=\"_\" + name2)\n",
    "    spread = joined.with_columns(\n",
    "        (pl.col(\"close\") / pl.col(f\"close_{name2}\")).alias(f\"{name1}_{name2}_spread\")\n",
    "    ).select([\"timestamp\", f\"{name1}_{name2}_spread\"])\n",
    "    return spread\n",
    "\n",
    "def compute_spread_of_spreads_df(spread_df1, spread_df2, name1, name2):\n",
    "    joined = spread_df1.join(spread_df2, on=\"timestamp\", how=\"inner\", suffix=\"_2\")\n",
    "    col1 = f\"{name1}_spread\"\n",
    "    col2 = f\"{name2}_spread\"\n",
    "    spread_of_spreads = joined.with_columns(\n",
    "        (pl.col(col1) / pl.col(col2)).alias(f\"{name1}_over_{name2}_spread\")\n",
    "    ).select([\"timestamp\", f\"{name1}_over_{name2}_spread\"])\n",
    "    return spread_of_spreads\n",
    "\n",
    "# Generate all spreads\n",
    "spread_dfs = {}\n",
    "symbols = list(dataframes.keys())\n",
    "\n",
    "print(\"Computing spreads...\")\n",
    "for sym1, sym2 in tqdm(list(combinations(symbols, 2)), desc=\"Computing spreads\"):\n",
    "    df1 = dataframes[sym1]\n",
    "    df2 = dataframes[sym2]\n",
    "    spread_name = f\"{sym1}_{sym2}\"\n",
    "    spread_df = compute_spread_df(df1, df2, sym1, sym2)\n",
    "    spread_dfs[spread_name] = spread_df\n",
    "\n",
    "# Generate all spread-of-spreads\n",
    "spread_of_spreads_dfs = {}\n",
    "\n",
    "print(\"Computing spread-of-spreads...\")\n",
    "for (spread1_name, spread2_name) in tqdm(list(combinations(spread_dfs.keys(), 2)), desc=\"Computing spread-of-spreads\"):\n",
    "    spread_df1 = spread_dfs[spread1_name]\n",
    "    spread_df2 = spread_dfs[spread2_name]\n",
    "    sos_name = f\"{spread1_name}_OVER_{spread2_name}\"\n",
    "    sos_df = compute_spread_of_spreads_df(spread_df1, spread_df2, spread1_name, spread2_name)\n",
    "    spread_of_spreads_dfs[sos_name] = sos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca45e5b",
   "metadata": {},
   "source": [
    "# Rolling correlation + cointegration [time series tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a3ca19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME SERIES FUNCTIONS / TESTS\n",
    "\n",
    "def adf_test(series):\n",
    "    \"\"\"\n",
    "    Perform the Augmented Dickey-Fuller (ADF) test to check for stationarity of a time series.\n",
    "    The ADF test tests the null hypothesis that the series has a unit root (i.e., is non-stationary).\n",
    "    If the p-value is below the significance level, we reject the null and conclude the series is stationary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(series) < MIN_OBSERVATIONS:\n",
    "            return None\n",
    "        clean_series = series[np.isfinite(series)]\n",
    "        if len(clean_series) < MIN_OBSERVATIONS:\n",
    "            return None\n",
    "        \n",
    "        # Use fewer lags for speed\n",
    "        result = adfuller(clean_series, maxlag=min(10, len(clean_series)//10), autolag='AIC')\n",
    "        return {\n",
    "            'p_value': result[1],\n",
    "            'is_stationary': result[1] < SIGNIFICANCE_LEVEL\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def kpss_test(series):\n",
    "    \"\"\"\n",
    "    Perform the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test to check for stationarity of a time series.\n",
    "    Unlike ADF, KPSS tests the null hypothesis that the series is stationary around a constant (or trend).\n",
    "    If the p-value is above the significance level, we fail to reject stationarity; otherwise, the series is non-stationary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(series) < MIN_OBSERVATIONS:\n",
    "            return None\n",
    "        clean_series = series[np.isfinite(series)]\n",
    "        if len(clean_series) < MIN_OBSERVATIONS:\n",
    "            return None\n",
    "        \n",
    "        result = kpss(clean_series, regression='c', nlags='auto')\n",
    "        return {\n",
    "            'p_value': result[1],\n",
    "            'is_stationary': result[1] > SIGNIFICANCE_LEVEL\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def correlation_test(series1, series2):\n",
    "    \"\"\"\n",
    "    Compute the Pearson correlation coefficient between two series.\n",
    "    This measures the linear relationship between the two series.\n",
    "    Returns the correlation value, p-value, and whether the correlation is statistically significant.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(series1) < MIN_OBSERVATIONS or len(series2) < MIN_OBSERVATIONS:\n",
    "            return None\n",
    "        \n",
    "        mask = np.isfinite(series1) & np.isfinite(series2)\n",
    "        clean_series1 = series1[mask]\n",
    "        clean_series2 = series2[mask]\n",
    "        \n",
    "        if len(clean_series1) < MIN_OBSERVATIONS:\n",
    "            return None\n",
    "        \n",
    "        corr, p_value = pearsonr(clean_series1, clean_series2)\n",
    "        return {\n",
    "            'correlation': corr,\n",
    "            'p_value': p_value,\n",
    "            'is_significant': p_value < SIGNIFICANCE_LEVEL\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def cointegration_test(series1, series2):\n",
    "    \"\"\"\n",
    "    Perform the Engle-Granger cointegration test to determine if two non-stationary series are cointegrated.\n",
    "    Cointegration implies a stable long-term relationship despite individual non-stationarity.\n",
    "    The null hypothesis is no cointegration; a low p-value means rejecting null, thus cointegrated.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(series1) < MIN_OBSERVATIONS or len(series2) < MIN_OBSERVATIONS:\n",
    "            return None\n",
    "        \n",
    "        mask = np.isfinite(series1) & np.isfinite(series2)\n",
    "        clean_series1 = series1[mask]\n",
    "        clean_series2 = series2[mask]\n",
    "        \n",
    "        if len(clean_series1) < MIN_OBSERVATIONS:\n",
    "            return None\n",
    "        \n",
    "        # Use fewer lags for speed\n",
    "        statistic, p_value, _ = coint(clean_series1, clean_series2)\n",
    "        return {\n",
    "            'p_value': p_value,\n",
    "            'is_cointegrated': p_value < SIGNIFICANCE_LEVEL\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_series_fast(df, column_name):\n",
    "    \"\"\"\n",
    "    Efficiently extract a column from a dataframe as a numpy array.\n",
    "    This speeds up numerical computations and tests on the series.\n",
    "    \"\"\"\n",
    "    return df.get_column(column_name).to_numpy()\n",
    "\n",
    "def time_series_analysis(df, series_name, column_name):\n",
    "    \"\"\"\n",
    "    Perform streamlined analysis on a single time series.\n",
    "    Returns basic statistics (mean, std), number of observations,\n",
    "    and results of stationarity tests (ADF and KPSS).\n",
    "    \"\"\"\n",
    "    series = extract_series_fast(df, column_name)\n",
    "    \n",
    "    return {\n",
    "        'series_name': series_name,\n",
    "        'n_observations': len(series),\n",
    "        'mean': float(np.mean(series)),\n",
    "        'std': float(np.std(series)),\n",
    "        'adf_test': adf_test(series),\n",
    "        'kpss_test': kpss_test(series)\n",
    "    }\n",
    "\n",
    "def pairwise_analysis(df1, df2, name1, name2, col1, col2):\n",
    "    \"\"\"\n",
    "    Perform pairwise analysis between two time series.\n",
    "    Computes correlation and cointegration tests, returning the results in a dictionary.\n",
    "    \"\"\"\n",
    "    series1 = extract_series_fast(df1, col1)\n",
    "    series2 = extract_series_fast(df2, col2)\n",
    "    \n",
    "    return {\n",
    "        'pair': f\"{name1}_vs_{name2}\",\n",
    "        'correlation': correlation_test(series1, series2),\n",
    "        'cointegration': cointegration_test(series1, series2)\n",
    "    }\n",
    "\n",
    "def analyze_all_spreads():\n",
    "    \"\"\"\n",
    "    Batch analysis of all spread time series.\n",
    "    Iterates over all spreads, performing stationarity and basic stats analysis.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Analyzing spreads...\")\n",
    "    for spread_name, spread_df in tqdm(spread_dfs.items(), desc=\"Spread analysis\"):\n",
    "        column_name = f\"{spread_name}_spread\"\n",
    "        results[spread_name] = time_series_analysis(spread_df, spread_name, column_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_all_sos():\n",
    "    \"\"\"\n",
    "    Batch analysis of all spread-of-spreads (SoS) time series.\n",
    "    Similar to spreads but for spread-of-spreads data.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Analyzing spread-of-spreads...\")\n",
    "    for sos_name, sos_df in tqdm(spread_of_spreads_dfs.items(), desc=\"SoS analysis\"):\n",
    "        column_name = f\"{sos_name.replace('_OVER_', '_over_')}_spread\"\n",
    "        results[sos_name] = time_series_analysis(sos_df, sos_name, column_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_spread_pairs():\n",
    "    \"\"\"\n",
    "    Batch pairwise analysis of all combinations of spreads.\n",
    "    Computes correlation and cointegration for every unique pair of spreads.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Pre-compute spread pairs to avoid repeated combinations\n",
    "    spread_names = list(spread_dfs.keys())\n",
    "    pairs = list(combinations(spread_names, 2))\n",
    "    \n",
    "    print(f\"Analyzing {len(pairs)} spread pairs...\")\n",
    "    for spread1_name, spread2_name in tqdm(pairs, desc=\"Spread pairs\"):\n",
    "        df1 = spread_dfs[spread1_name]\n",
    "        df2 = spread_dfs[spread2_name]\n",
    "        col1 = f\"{spread1_name}_spread\"\n",
    "        col2 = f\"{spread2_name}_spread\"\n",
    "        \n",
    "        pair_key = f\"{spread1_name}_vs_{spread2_name}\"\n",
    "        results[pair_key] = pairwise_analysis(df1, df2, spread1_name, spread2_name, col1, col2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Pre-compute series data for faster access\n",
    "def precompute_series_data():\n",
    "    \"\"\"\n",
    "    Pre-extract all spread and spread-of-spread series data into numpy arrays.\n",
    "    This allows faster repeated analysis without repeated dataframe extraction overhead.\n",
    "    \"\"\"\n",
    "    spread_series = {}\n",
    "    sos_series = {}\n",
    "    \n",
    "    print(\"Pre-computing series data...\")\n",
    "    # Extract spread series\n",
    "    for spread_name, spread_df in tqdm(spread_dfs.items(), desc=\"Extracting spreads\"):\n",
    "        column_name = f\"{spread_name}_spread\"\n",
    "        spread_series[spread_name] = extract_series_fast(spread_df, column_name)\n",
    "    \n",
    "    # Extract SoS series  \n",
    "    for sos_name, sos_df in tqdm(spread_of_spreads_dfs.items(), desc=\"Extracting SoS\"):\n",
    "        column_name = f\"{sos_name.replace('_OVER_', '_over_')}_spread\"\n",
    "        sos_series[sos_name] = extract_series_fast(sos_df, column_name)\n",
    "    \n",
    "    return spread_series, sos_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe12a57",
   "metadata": {},
   "source": [
    "## Time series analysis of spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c07185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing series data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting spreads: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "Extracting SoS: 100%|██████████| 15/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing spreads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spread analysis: 100%|██████████| 6/6 [00:11<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing spread-of-spreads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoS analysis: 100%|██████████| 15/15 [00:16<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 15 spread pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spread pairs: 100%|██████████| 15/15 [00:00<00:00, 1662.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STATIONARITY SUMMARY\n",
      "============================================================\n",
      "STATIONARY SPREADS (0):\n",
      "\n",
      "NON-STATIONARY SPREADS (6):\n",
      "  - BTCUSD_ETHUSD\n",
      "  - BTCUSD_SOLUSD\n",
      "  - BTCUSD_XRPUSD\n",
      "  - ETHUSD_SOLUSD\n",
      "  - ETHUSD_XRPUSD\n",
      "  - SOLUSD_XRPUSD\n",
      "\n",
      "============================================================\n",
      "HIGH CORRELATIONS (|r| > 0.8)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "COINTEGRATED PAIRS\n",
      "============================================================\n",
      "  No cointegrated pairs found\n",
      "\n",
      "============================================================\n",
      "DETAILED STATISTICS\n",
      "============================================================\n",
      "Number of spreads analyzed: 6\n",
      "Number of spread-of-spreads analyzed: 15\n",
      "Number of pairwise comparisons: 15\n",
      "\n",
      "Observation counts:\n",
      "  Min: 220,492\n",
      "  Max: 958,426\n",
      "  Mean: 416,056\n",
      "\n",
      "Test success rates:\n",
      "  ADF tests completed: 6/6 (100.0%)\n",
      "  KPSS tests completed: 6/6 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute all series data\n",
    "spread_series_data, sos_series_data = precompute_series_data()\n",
    "\n",
    "# Analyze individual spreads\n",
    "spread_results = analyze_all_spreads()\n",
    "\n",
    "# Analyze spread-of-spreads\n",
    "sos_results = analyze_all_sos()\n",
    "\n",
    "# Analyze pairwise relationships\n",
    "pairwise_results = analyze_spread_pairs()\n",
    "\n",
    "def display_summary_results():\n",
    "    \"\"\"Display condensed results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STATIONARITY SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    stationary_spreads = []\n",
    "    non_stationary_spreads = []\n",
    "    \n",
    "    for name, results in spread_results.items():\n",
    "        adf_stat = results['adf_test']['is_stationary'] if results['adf_test'] else False\n",
    "        kpss_stat = results['kpss_test']['is_stationary'] if results['kpss_test'] else False\n",
    "        \n",
    "        consensus = adf_stat and kpss_stat\n",
    "        if consensus:\n",
    "            stationary_spreads.append(name)\n",
    "        else:\n",
    "            non_stationary_spreads.append(name)\n",
    "    \n",
    "    print(f\"STATIONARY SPREADS ({len(stationary_spreads)}):\")\n",
    "    for spread in stationary_spreads:\n",
    "        print(f\"  - {spread}\")\n",
    "    \n",
    "    print(f\"\\nNON-STATIONARY SPREADS ({len(non_stationary_spreads)}):\")\n",
    "    for spread in non_stationary_spreads[:10]:  # Show first 10\n",
    "        print(f\"  - {spread}\")\n",
    "    if len(non_stationary_spreads) > 10:\n",
    "        print(f\"  ... and {len(non_stationary_spreads) - 10} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HIGH CORRELATIONS (|r| > 0.8)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    high_corr = []\n",
    "    for pair_name, results in pairwise_results.items():\n",
    "        if results['correlation'] and abs(results['correlation']['correlation']) > 0.8:\n",
    "            high_corr.append((\n",
    "                results['pair'], \n",
    "                results['correlation']['correlation']\n",
    "            ))\n",
    "    \n",
    "    high_corr.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    for pair, corr in high_corr[:15]:  # Show top 15\n",
    "        print(f\"  {pair:<35} r = {corr:7.4f}\")\n",
    "    \n",
    "    if len(high_corr) > 15:\n",
    "        print(f\"  ... and {len(high_corr) - 15} more pairs\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COINTEGRATED PAIRS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    coint_pairs = []\n",
    "    for pair_name, results in pairwise_results.items():\n",
    "        if results['cointegration'] and results['cointegration']['is_cointegrated']:\n",
    "            coint_pairs.append((\n",
    "                results['pair'],\n",
    "                results['cointegration']['p_value']\n",
    "            ))\n",
    "    \n",
    "    coint_pairs.sort(key=lambda x: x[1])\n",
    "    \n",
    "    if coint_pairs:\n",
    "        for pair, p_val in coint_pairs[:10]:  # Show top 10\n",
    "            print(f\"  {pair:<35} p = {p_val:.6f}\")\n",
    "        if len(coint_pairs) > 10:\n",
    "            print(f\"  ... and {len(coint_pairs) - 10} more pairs\")\n",
    "    else:\n",
    "        print(\"  No cointegrated pairs found\")\n",
    "\n",
    "def display_detailed_stats():\n",
    "    \"\"\"Display key statistics\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Spread statistics\n",
    "    print(f\"Number of spreads analyzed: {len(spread_results)}\")\n",
    "    print(f\"Number of spread-of-spreads analyzed: {len(sos_results)}\")\n",
    "    print(f\"Number of pairwise comparisons: {len(pairwise_results)}\")\n",
    "    \n",
    "    # Observation counts\n",
    "    obs_counts = [r['n_observations'] for r in spread_results.values()]\n",
    "    print(f\"\\nObservation counts:\")\n",
    "    print(f\"  Min: {min(obs_counts):,}\")\n",
    "    print(f\"  Max: {max(obs_counts):,}\")\n",
    "    print(f\"  Mean: {np.mean(obs_counts):,.0f}\")\n",
    "    \n",
    "    # Test success rates\n",
    "    adf_success = sum(1 for r in spread_results.values() if r['adf_test'] is not None)\n",
    "    kpss_success = sum(1 for r in spread_results.values() if r['kpss_test'] is not None)\n",
    "    \n",
    "    print(f\"\\nTest success rates:\")\n",
    "    print(f\"  ADF tests completed: {adf_success}/{len(spread_results)} ({100*adf_success/len(spread_results):.1f}%)\")\n",
    "    print(f\"  KPSS tests completed: {kpss_success}/{len(spread_results)} ({100*kpss_success/len(spread_results):.1f}%)\")\n",
    "    \n",
    "    # Correlation distribution\n",
    "    correlations = [r['correlation']['correlation'] for r in pairwise_results.values() \n",
    "                   if r['correlation'] is not None]\n",
    "    \n",
    "    if correlations:\n",
    "        print(f\"\\nCorrelation distribution:\")\n",
    "        print(f\"  Mean: {np.mean(correlations):.4f}\")\n",
    "        print(f\"  Std: {np.std(correlations):.4f}\")\n",
    "        print(f\"  |r| > 0.5: {sum(1 for c in correlations if abs(c) > 0.5)} pairs\")\n",
    "        print(f\"  |r| > 0.8: {sum(1 for c in correlations if abs(c) > 0.8)} pairs\")\n",
    "\n",
    "# Display results\n",
    "display_summary_results()\n",
    "display_detailed_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a3172",
   "metadata": {},
   "source": [
    "None of the spreads or spread-of-spreads showed signs of being stationary, highly correlated, or cointegrated, suggesting weak or unstable long-term statistical relationships between the crypto pairs’ price ratios. Standard mean reversion strategies might not work long-term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8663a9",
   "metadata": {},
   "source": [
    "## Clustering-based Arbitrage (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8889f9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clustering analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning spreads for clustering: 100%|██████████| 6/6 [00:00<00:00, 166.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering finished. Cluster assignments:\n",
      "  BTCUSD_ETHUSD: Cluster 1\n",
      "  BTCUSD_SOLUSD: Cluster 1\n",
      "  BTCUSD_XRPUSD: Cluster 1\n",
      "  ETHUSD_SOLUSD: Cluster 1\n",
      "  ETHUSD_XRPUSD: Cluster 1\n",
      "  SOLUSD_XRPUSD: Cluster 1\n",
      "\n",
      "Co-clustered pairs for potential arbitrage:\n",
      "  - BTCUSD_ETHUSD and BTCUSD_SOLUSD\n",
      "  - BTCUSD_ETHUSD and BTCUSD_XRPUSD\n",
      "  - BTCUSD_ETHUSD and ETHUSD_SOLUSD\n",
      "  - BTCUSD_ETHUSD and ETHUSD_XRPUSD\n",
      "  - BTCUSD_ETHUSD and SOLUSD_XRPUSD\n",
      "  - BTCUSD_SOLUSD and BTCUSD_XRPUSD\n",
      "  - BTCUSD_SOLUSD and ETHUSD_SOLUSD\n",
      "  - BTCUSD_SOLUSD and ETHUSD_XRPUSD\n",
      "  - BTCUSD_SOLUSD and SOLUSD_XRPUSD\n",
      "  - BTCUSD_XRPUSD and ETHUSD_SOLUSD\n",
      "  - BTCUSD_XRPUSD and ETHUSD_XRPUSD\n",
      "  - BTCUSD_XRPUSD and SOLUSD_XRPUSD\n",
      "  - ETHUSD_SOLUSD and ETHUSD_XRPUSD\n",
      "  - ETHUSD_SOLUSD and SOLUSD_XRPUSD\n",
      "  - ETHUSD_XRPUSD and SOLUSD_XRPUSD\n",
      "\n",
      "Starting PCA analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning prices for PCA: 100%|██████████| 4/4 [00:00<00:00, 74.09it/s]\n",
      "Fitting Incremental PCA: 100%|██████████| 862/862 [00:00<00:00, 3096.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ADF Test on PC1: p-value = 0.784579, stationary = False\n",
      "\n",
      "PC1 Loadings (sorted):\n",
      "  SOLUSD                   :  0.5596\n",
      "  BTCUSD                   :  0.5457\n",
      "  ETHUSD                   :  0.4811\n",
      "  XRPUSD                   :  0.3970\n",
      "\n",
      "Candidate pairs from PCA:\n",
      "  - SOLUSD and SOLUSD\n",
      "  - SOLUSD and BTCUSD\n",
      "  - SOLUSD and ETHUSD\n",
      "  - SOLUSD and XRPUSD\n",
      "  - BTCUSD and SOLUSD\n",
      "  - BTCUSD and BTCUSD\n",
      "  - BTCUSD and ETHUSD\n",
      "  - BTCUSD and XRPUSD\n",
      "  - ETHUSD and SOLUSD\n",
      "  - ETHUSD and BTCUSD\n",
      "  - ETHUSD and ETHUSD\n",
      "  - ETHUSD and XRPUSD\n",
      "  - XRPUSD and SOLUSD\n",
      "  - XRPUSD and BTCUSD\n",
      "  - XRPUSD and ETHUSD\n",
      "  - XRPUSD and XRPUSD\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "def cluster_with_kmeans(data_df, n_clusters=4, batch_size=256):\n",
    "    \"\"\"\n",
    "    Cluster assets based on their price movements using K-Means.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pl.DataFrame): A Polars DataFrame with assets as columns and timestamps as rows.\n",
    "        n_clusters (int): The number of clusters to form.\n",
    "        batch_size (int): Batch size for MiniBatchKMeans.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained MiniBatchKMeans model and the fitted scaler.\n",
    "    \"\"\"\n",
    "    if data_df.height < 2:\n",
    "        print(\"Not enough data to cluster.\")\n",
    "        return None, None\n",
    "        \n",
    "    # Standardize the data\n",
    "    # Polars doesn't have a direct StandardScaler equivalent, so we do it manually\n",
    "    df_mean = data_df.mean().row(0)\n",
    "    df_std = data_df.std().row(0)\n",
    "    \n",
    "    # Check for zero standard deviation, which can cause division by zero\n",
    "    if any(s == 0 for s in df_std):\n",
    "        print(\"Warning: One or more series have zero standard deviation. Skipping clustering.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Normalize the data using Polars expressions\n",
    "    normalized_df = data_df.select([(pl.col(c) - df_mean[i]) / df_std[i] for i, c in enumerate(data_df.columns)])\n",
    "\n",
    "    # Convert to NumPy for scikit-learn\n",
    "    X = normalized_df.to_numpy()\n",
    "    \n",
    "    # Use MiniBatchKMeans for faster computation on large datasets\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, n_init=3, batch_size=batch_size)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    return kmeans, (df_mean, df_std)\n",
    "\n",
    "def find_cluster_pairs(cluster_labels):\n",
    "    \"\"\"\n",
    "    Find pairs within the same cluster for potential arbitrage.\n",
    "    \n",
    "    Args:\n",
    "        cluster_labels (dict): Dictionary mapping asset names to cluster labels.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains two asset names from the same cluster.\n",
    "    \"\"\"\n",
    "    cluster_map = {}\n",
    "    for asset, cluster_id in cluster_labels.items():\n",
    "        if cluster_id not in cluster_map:\n",
    "            cluster_map[cluster_id] = []\n",
    "        cluster_map[cluster_id].append(asset)\n",
    "        \n",
    "    pairs = []\n",
    "    for cluster_id, assets in cluster_map.items():\n",
    "        if len(assets) >= 2:\n",
    "            pairs.extend(list(combinations(assets, 2)))\n",
    "    return pairs\n",
    "\n",
    "def plot_cluster_results(df, cluster_labels, title=\"Clustering Results\"):\n",
    "    \"\"\"\n",
    "    Visualize clustering results.\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): Input data used for clustering.\n",
    "        cluster_labels (np.ndarray): The cluster labels for each data point.\n",
    "        title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        return\n",
    "    \n",
    "    # Convert Polars DataFrame to Pandas for easy plotting\n",
    "    pdf = df.to_pandas()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, col in enumerate(pdf.columns):\n",
    "        plt.scatter(pdf.index, pdf[col], c=[cluster_labels[i]], label=f'Cluster {cluster_labels[i]}', alpha=0.5)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.show()\n",
    "\n",
    "# Main clustering logic\n",
    "print(\"Starting clustering analysis...\")\n",
    "# We'll use the spreads for clustering as they represent relationships\n",
    "spreads_for_clustering = pl.DataFrame()\n",
    "first_spread = True\n",
    "\n",
    "# Align all spread dataframes on a common timestamp\n",
    "for name, df in tqdm(spread_dfs.items(), desc=\"Aligning spreads for clustering\"):\n",
    "    df = df.sort(\"timestamp\")\n",
    "    if first_spread:\n",
    "        spreads_for_clustering = df.rename({f\"{name}_spread\": name})\n",
    "        first_spread = False\n",
    "    else:\n",
    "        spreads_for_clustering = spreads_for_clustering.join(df.rename({f\"{name}_spread\": name}), on=\"timestamp\", how=\"inner\")\n",
    "\n",
    "if spreads_for_clustering.is_empty():\n",
    "    print(\"No common data across spreads. Cannot perform clustering.\")\n",
    "else:\n",
    "    # Use MiniBatchKMeans for scalability\n",
    "    kmeans_model, scaler = cluster_with_kmeans(spreads_for_clustering.select(pl.all().exclude(\"timestamp\")))\n",
    "    if kmeans_model:\n",
    "        cluster_labels = dict(zip(spreads_for_clustering.columns[1:], kmeans_model.labels_))\n",
    "        print(\"Clustering finished. Cluster assignments:\")\n",
    "        for asset, label in cluster_labels.items():\n",
    "            print(f\"  {asset}: Cluster {label}\")\n",
    "            \n",
    "        co_clustered_pairs = find_cluster_pairs(cluster_labels)\n",
    "        if co_clustered_pairs:\n",
    "            print(\"\\nCo-clustered pairs for potential arbitrage:\")\n",
    "            for pair in co_clustered_pairs:\n",
    "                print(f\"  - {pair[0]} and {pair[1]}\")\n",
    "                \n",
    "        # Optional: Plotting results\n",
    "        # A simple way to visualize is to plot the time series with color-coded clusters\n",
    "        # Note: This is computationally intensive for large datasets.\n",
    "        # plot_cluster_results(spreads_for_clustering.select(pl.all().exclude(\"timestamp\")), kmeans_model.labels_)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## PCA-based Arbitrage (Principal Component Analysis)\n",
    "\n",
    "# %%\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "def pca_based_arbitrage(data_df, n_components=1, batch_size=256):\n",
    "    \"\"\"\n",
    "    Perform PCA to find the principal components of asset prices.\n",
    "    The first principal component often represents the market's common trend.\n",
    "    Relative value can be sought by trading deviations from this trend.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pl.DataFrame): A Polars DataFrame with assets as columns and timestamps as rows.\n",
    "        n_components (int): Number of principal components to keep.\n",
    "        batch_size (int): Batch size for IncrementalPCA.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the fitted IncrementalPCA model and the transformed data.\n",
    "    \"\"\"\n",
    "    if data_df.height < n_components + 1:\n",
    "        print(\"Not enough data points for PCA with the given number of components.\")\n",
    "        return None, None\n",
    "        \n",
    "    # Manually standardize the data\n",
    "    df_mean = data_df.mean().row(0)\n",
    "    df_std = data_df.std().row(0)\n",
    "    \n",
    "    if any(s == 0 for s in df_std):\n",
    "        print(\"Warning: One or more series have zero standard deviation. Skipping PCA.\")\n",
    "        return None, None\n",
    "        \n",
    "    normalized_df = data_df.select([(pl.col(c) - df_mean[i]) / df_std[i] for i, c in enumerate(data_df.columns)])\n",
    "    \n",
    "    # Use IncrementalPCA for large datasets\n",
    "    ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "    X = normalized_df.to_numpy()\n",
    "    \n",
    "    # Partial fit in chunks\n",
    "    for i in tqdm(range(0, X.shape[0], batch_size), desc=\"Fitting Incremental PCA\"):\n",
    "        ipca.partial_fit(X[i:i + batch_size, :])\n",
    "    \n",
    "    # Transform the data\n",
    "    transformed_data = ipca.transform(X)\n",
    "    \n",
    "    return ipca, transformed_data\n",
    "\n",
    "def find_pca_pairs(transformed_data, ipca_model, asset_names):\n",
    "    \"\"\"\n",
    "    Find potential pairs for PCA-based arbitrage by analyzing component loadings.\n",
    "    Pairs with similar and high loadings on a principal component are good candidates.\n",
    "    \n",
    "    Args:\n",
    "        transformed_data (np.ndarray): The data projected onto the principal components.\n",
    "        ipca_model (IncrementalPCA): The fitted PCA model.\n",
    "        asset_names (list): List of asset names corresponding to columns.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples containing candidate pairs.\n",
    "    \"\"\"\n",
    "    # The first principal component (PC1) often represents the market trend.\n",
    "    # The loadings (components_) show the weight of each asset on that component.\n",
    "    pc1_loadings = ipca_model.components_[0]\n",
    "    \n",
    "    # Sort assets by their loading on the first component\n",
    "    sorted_assets = sorted(zip(asset_names, pc1_loadings), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nPC1 Loadings (sorted):\")\n",
    "    for name, loading in sorted_assets:\n",
    "        print(f\"  {name:<25}: {loading:7.4f}\")\n",
    "        \n",
    "    # Find pairs with high correlation to PC1\n",
    "    # This is a simplified approach; a more advanced method would analyze\n",
    "    # residuals (the part of the series not explained by PC1).\n",
    "    \n",
    "    # Identify the top and bottom assets by loading\n",
    "    # Top candidates are those with highest positive loadings and lowest negative loadings\n",
    "    num_candidates = min(5, len(sorted_assets))\n",
    "    top_positive = sorted_assets[:num_candidates]\n",
    "    top_negative = sorted_assets[-num_candidates:]\n",
    "    \n",
    "    candidate_pairs = []\n",
    "    # Arbitrage candidates are often between a high-loading asset and a low-loading one\n",
    "    # Or between two assets with high but opposite loadings\n",
    "    \n",
    "    # Example: pairing top positive with top negative\n",
    "    for pos_asset, _ in top_positive:\n",
    "        for neg_asset, _ in top_negative:\n",
    "            candidate_pairs.append((pos_asset, neg_asset))\n",
    "            \n",
    "    return candidate_pairs\n",
    "\n",
    "# Main PCA logic\n",
    "print(\"\\nStarting PCA analysis...\")\n",
    "# We use the raw crypto prices for PCA to find a common market factor\n",
    "price_data = pl.DataFrame()\n",
    "first_asset = True\n",
    "\n",
    "# Align raw price data on a common timestamp\n",
    "for symbol in tqdm(SYMBOLS, desc=\"Aligning prices for PCA\"):\n",
    "    df = dataframes[symbol].sort(\"timestamp\").select([\"timestamp\", \"close\"]).rename({\"close\": symbol})\n",
    "    if first_asset:\n",
    "        price_data = df\n",
    "        first_asset = False\n",
    "    else:\n",
    "        price_data = price_data.join(df, on=\"timestamp\", how=\"inner\")\n",
    "\n",
    "if price_data.is_empty():\n",
    "    print(\"No common data across assets. Cannot perform PCA.\")\n",
    "else:\n",
    "    # Use a subset of the data for faster testing if needed\n",
    "    # price_data_subset = price_data.tail(100000)\n",
    "    \n",
    "    ipca_model, transformed_data = pca_based_arbitrage(price_data.select(pl.all().exclude(\"timestamp\")))\n",
    "    \n",
    "    if ipca_model:\n",
    "        pc1 = transformed_data[:, 0]\n",
    "        # Check for stationarity of the first principal component\n",
    "        pc1_adf = adf_test(pc1)\n",
    "        print(f\"\\nADF Test on PC1: p-value = {pc1_adf['p_value']:.6f}, stationary = {pc1_adf['is_stationary']}\")\n",
    "        \n",
    "        # Analyze loadings to find arbitrage candidates\n",
    "        asset_names = price_data.columns[1:]\n",
    "        pca_pairs = find_pca_pairs(transformed_data, ipca_model, asset_names)\n",
    "        \n",
    "        print(\"\\nCandidate pairs from PCA:\")\n",
    "        for pair in pca_pairs:\n",
    "            print(f\"  - {pair[0]} and {pair[1]}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060d577",
   "metadata": {},
   "source": [
    "\n",
    "## Hidden Markov Models (HMM) for Market Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04ed0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting HMM analysis on spreads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HMM Analysis:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HMM for BTCUSD_ETHUSD on the last 50000 observations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HMM Analysis:  50%|█████     | 1/2 [00:01<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HMM Analysis for BTCUSD_ETHUSD:\n",
      "  Number of hidden states: 2\n",
      "  State 0:\n",
      "    Mean: -0.000085\n",
      "    Std Dev: 0.004008\n",
      "    Interpretation: Mean-Reverting\n",
      "  State 1:\n",
      "    Mean: 0.000001\n",
      "    Std Dev: 0.001130\n",
      "    Interpretation: Mean-Reverting\n",
      "\n",
      "  Transition Matrix:\n",
      "         State 0  State 1\n",
      "State 0   0.8189   0.1811\n",
      "State 1   0.0179   0.9821\n",
      "\n",
      "  Longest consecutive runs:\n",
      "    State 0: max length 74\n",
      "    State 1: max length 1343\n",
      "\n",
      "Training HMM for SOLUSD_XRPUSD on the last 50000 observations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HMM Analysis: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HMM Analysis for SOLUSD_XRPUSD:\n",
      "  Number of hidden states: 2\n",
      "  State 0:\n",
      "    Mean: -0.000026\n",
      "    Std Dev: 0.004214\n",
      "    Interpretation: Mean-Reverting\n",
      "  State 1:\n",
      "    Mean: 0.000004\n",
      "    Std Dev: 0.001400\n",
      "    Interpretation: Mean-Reverting\n",
      "\n",
      "  Transition Matrix:\n",
      "         State 0  State 1\n",
      "State 0   0.9196   0.0804\n",
      "State 1   0.0103   0.9897\n",
      "\n",
      "  Longest consecutive runs:\n",
      "    State 0: max length 208\n",
      "    State 1: max length 2391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from hmmlearn import hmm\n",
    "\n",
    "def train_hmm(data_series, n_states=2, n_iter=100, tol=0.01):\n",
    "    \"\"\"\n",
    "    Train a Hidden Markov Model to identify market regimes.\n",
    "    \n",
    "    Args:\n",
    "        data_series (np.ndarray): The time series data, typically daily returns or log returns.\n",
    "        n_states (int): The number of hidden states (e.g., trending, mean-reverting).\n",
    "        n_iter (int): The number of iterations for the EM algorithm.\n",
    "        tol (float): The convergence tolerance.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: The trained HMM model and the fitted states.\n",
    "    \"\"\"\n",
    "    if len(data_series) < 100:\n",
    "        print(\"Not enough data to train HMM.\")\n",
    "        return None, None\n",
    "        \n",
    "    # hmmlearn expects a 2D array, so we reshape it\n",
    "    X = data_series.reshape(-1, 1)\n",
    "    \n",
    "    # We use a GaussianHMM since returns are often modeled with a Gaussian distribution\n",
    "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\", n_iter=n_iter, tol=tol)\n",
    "    \n",
    "    try:\n",
    "        model.fit(X)\n",
    "        hidden_states = model.predict(X)\n",
    "    except Exception as e:\n",
    "        print(f\"HMM training failed: {e}\")\n",
    "        return None, None\n",
    "        \n",
    "    return model, hidden_states\n",
    "\n",
    "def analyze_hmm_results(model, hidden_states, series_name):\n",
    "    \"\"\"\n",
    "    Analyze and print the results of the HMM.\n",
    "    \n",
    "    Args:\n",
    "        model (hmm.GaussianHMM): The trained HMM model.\n",
    "        hidden_states (np.ndarray): The sequence of predicted hidden states.\n",
    "        series_name (str): The name of the series being analyzed.\n",
    "    \"\"\"\n",
    "    if not model:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nHMM Analysis for {series_name}:\")\n",
    "    print(f\"  Number of hidden states: {model.n_components}\")\n",
    "    \n",
    "    for i in range(model.n_components):\n",
    "        state_mean = model.means_[i][0]\n",
    "        state_cov = np.sqrt(model.covars_[i][0][0])\n",
    "        \n",
    "        # A simple interpretation: a trending state has a non-zero mean return,\n",
    "        # while a mean-reverting state has a mean close to zero.\n",
    "        # This is a heuristic; more sophisticated analysis is needed.\n",
    "        state_type = \"Trending\" if abs(state_mean) > 1e-4 else \"Mean-Reverting\"\n",
    "        print(f\"  State {i}:\")\n",
    "        print(f\"    Mean: {state_mean:.6f}\")\n",
    "        print(f\"    Std Dev: {state_cov:.6f}\")\n",
    "        print(f\"    Interpretation: {state_type}\")\n",
    "        \n",
    "    # Print transition matrix\n",
    "    print(\"\\n  Transition Matrix:\")\n",
    "    transition_df = pd.DataFrame(model.transmat_, index=[f\"State {i}\" for i in range(model.n_components)], columns=[f\"State {i}\" for i in range(model.n_components)])\n",
    "    print(transition_df.round(4))\n",
    "    \n",
    "    # Find the longest consecutive run in each state\n",
    "    state_lengths = {}\n",
    "    for i in range(model.n_components):\n",
    "        state_lengths[i] = []\n",
    "        current_length = 0\n",
    "        for state in hidden_states:\n",
    "            if state == i:\n",
    "                current_length += 1\n",
    "            else:\n",
    "                if current_length > 0:\n",
    "                    state_lengths[i].append(current_length)\n",
    "                current_length = 0\n",
    "        if current_length > 0:\n",
    "            state_lengths[i].append(current_length)\n",
    "            \n",
    "    print(\"\\n  Longest consecutive runs:\")\n",
    "    for state, lengths in state_lengths.items():\n",
    "        if lengths:\n",
    "            print(f\"    State {state}: max length {max(lengths)}\")\n",
    "\n",
    "# Main HMM logic\n",
    "print(\"\\nStarting HMM analysis on spreads...\")\n",
    "# We will analyze a few key spreads\n",
    "spreads_to_analyze = ['BTCUSD_ETHUSD', 'SOLUSD_XRPUSD']\n",
    "\n",
    "for spread_name in tqdm(spreads_to_analyze, desc=\"HMM Analysis\"):\n",
    "    if spread_name in spread_dfs:\n",
    "        spread_df = spread_dfs[spread_name].sort(\"timestamp\")\n",
    "        \n",
    "        # Compute log returns of the spread\n",
    "        spread_log_returns = np.log(spread_df.get_column(f\"{spread_name}_spread\").to_numpy() / spread_df.get_column(f\"{spread_name}_spread\").shift(1).to_numpy())\n",
    "        \n",
    "        # Drop the first NaN value\n",
    "        spread_log_returns = spread_log_returns[1:]\n",
    "        \n",
    "        # Use a rolling window for a more practical analysis\n",
    "        window_size = 50000\n",
    "        if len(spread_log_returns) < window_size:\n",
    "            print(f\"Skipping {spread_name}: not enough data for window size {window_size}.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nTraining HMM for {spread_name} on the last {window_size} observations.\")\n",
    "        hmm_model, hidden_states = train_hmm(spread_log_returns[-window_size:])\n",
    "        \n",
    "        if hmm_model:\n",
    "            analyze_hmm_results(hmm_model, hidden_states, spread_name)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2361c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
