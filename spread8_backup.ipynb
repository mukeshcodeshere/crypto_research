{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d193a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generalized Volume-Enhanced Cryptocurrency Cointegration Analysis\n",
    "Analyzes cointegration between any two cryptocurrency pairs with volume regime detection\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "class CryptoCointegrationAnalyzer:\n",
    "    \"\"\"\n",
    "    Generalized cointegration analyzer for any two cryptocurrency pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, symbol1, symbol2, data_dir=\"coinbase/5m\"):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with two crypto symbols.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol1, symbol2 : str\n",
    "            Cryptocurrency symbols (e.g., 'BTCUSD', 'ETHUSD')\n",
    "        data_dir : str or Path\n",
    "            Directory containing historical data\n",
    "        \"\"\"\n",
    "        self.symbol1 = symbol1\n",
    "        self.symbol2 = symbol2\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data = {}\n",
    "        self.results = {}\n",
    "        \n",
    "        print(f\"Initialized analyzer for {symbol1} vs {symbol2}\")\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and prepare data for both symbols.\"\"\"\n",
    "        print(f\"\\nLoading data for {self.symbol1} and {self.symbol2}...\")\n",
    "        \n",
    "        csv_columns = [\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "        \n",
    "        for symbol in [self.symbol1, self.symbol2]:\n",
    "            print(f\"\\nProcessing {symbol}:\")\n",
    "            yearly_dfs = []\n",
    "            \n",
    "            for year_dir in sorted(self.data_dir.iterdir()):\n",
    "                if not year_dir.is_dir():\n",
    "                    continue\n",
    "                    \n",
    "                file_path = year_dir / f\"{symbol}_5m_{year_dir.name}.csv\"\n",
    "                if file_path.exists():\n",
    "                    df = pl.read_csv(file_path, has_header=False, new_columns=csv_columns)\n",
    "                    \n",
    "                    df_clean = df.filter(\n",
    "                        pl.col(\"close\").is_not_null() &\n",
    "                        pl.col(\"volume\").is_not_null() &\n",
    "                        (pl.col(\"close\") > 0) &\n",
    "                        (pl.col(\"volume\") >= 0) &\n",
    "                        pl.col(\"close\").is_finite() &\n",
    "                        pl.col(\"volume\").is_finite()\n",
    "                    )\n",
    "                    \n",
    "                    yearly_dfs.append(df_clean)\n",
    "                    print(f\"  {year_dir.name}: {df_clean.height:,} valid observations\")\n",
    "            \n",
    "            if not yearly_dfs:\n",
    "                raise ValueError(f\"No data found for {symbol} in {self.data_dir}\")\n",
    "            \n",
    "            combined_df = pl.concat(yearly_dfs)\n",
    "            combined_df = combined_df.with_columns(\n",
    "                (pl.col(\"timestamp\") * 1000).cast(pl.Datetime(\"ms\")).alias(\"datetime\")\n",
    "            ).sort(\"datetime\")\n",
    "            \n",
    "            combined_df = combined_df.unique(subset=[\"datetime\"], keep=\"first\")\n",
    "            self.data[symbol] = combined_df\n",
    "            \n",
    "            print(f\"  Total observations: {combined_df.height:,}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_timeframe_aggregations(self, timeframes):\n",
    "        \"\"\"\n",
    "        Aggregate 5-minute data to multiple timeframes.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        timeframes : dict\n",
    "            Dictionary of {label: frequency} (e.g., {'1H': '1h', '4H': '4h'})\n",
    "        \"\"\"\n",
    "        print(f\"\\nCreating multi-timeframe datasets...\")\n",
    "        self.timeframe_data = {}\n",
    "        \n",
    "        for tf_label, tf_freq in timeframes.items():\n",
    "            sym1_agg = self.data[self.symbol1].group_by_dynamic(\"datetime\", every=tf_freq).agg([\n",
    "                pl.col(\"open\").first().alias(\"open\"),\n",
    "                pl.col(\"high\").max().alias(\"high\"),\n",
    "                pl.col(\"low\").min().alias(\"low\"),\n",
    "                pl.col(\"close\").last().alias(\"close\"),\n",
    "                pl.col(\"volume\").sum().alias(\"volume\")\n",
    "            ]).filter(pl.col(\"volume\") > 0).sort(\"datetime\")\n",
    "            \n",
    "            sym2_agg = self.data[self.symbol2].group_by_dynamic(\"datetime\", every=tf_freq).agg([\n",
    "                pl.col(\"open\").first().alias(\"open\"),\n",
    "                pl.col(\"high\").max().alias(\"high\"),\n",
    "                pl.col(\"low\").min().alias(\"low\"), \n",
    "                pl.col(\"close\").last().alias(\"close\"),\n",
    "                pl.col(\"volume\").sum().alias(\"volume\")\n",
    "            ]).filter(pl.col(\"volume\") > 0).sort(\"datetime\")\n",
    "            \n",
    "            self.timeframe_data[tf_label] = {\n",
    "                self.symbol1: sym1_agg, \n",
    "                self.symbol2: sym2_agg\n",
    "            }\n",
    "            print(f\"  {tf_label:>5s}: {self.symbol1}={sym1_agg.height:,} obs, {self.symbol2}={sym2_agg.height:,} obs\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def calculate_spread_formulations(self, prices1, prices2):\n",
    "        \"\"\"Calculate multiple spread formulations.\"\"\"\n",
    "        spreads = {}\n",
    "\n",
    "        # 1. Linear Spread\n",
    "        range1 = np.ptp(prices1)\n",
    "        range2 = np.ptp(prices2)\n",
    "        scale_ratio = range1 / range2 if range2 > 0 else np.inf\n",
    "\n",
    "        if scale_ratio < 10:\n",
    "            spreads['linear'] = {\n",
    "                'values': prices1 - prices2,\n",
    "                'description': 'Linear Price Difference',\n",
    "                'formula': f'P_{self.symbol1} - P_{self.symbol2}'\n",
    "            }\n",
    "\n",
    "        # 2. Log Spread\n",
    "        spreads['log'] = {\n",
    "            'values': np.log(prices1) - np.log(prices2),\n",
    "            'description': 'Log Price Difference',\n",
    "            'formula': f'log(P_{self.symbol1}) - log(P_{self.symbol2})'\n",
    "        }\n",
    "\n",
    "        # 3. Ratio Spread\n",
    "        spreads['ratio'] = {\n",
    "            'values': prices1 / prices2,\n",
    "            'description': 'Price Ratio',\n",
    "            'formula': f'P_{self.symbol1} / P_{self.symbol2}'\n",
    "        }\n",
    "\n",
    "        # 4. Residual-based spread (OLS)\n",
    "        try:\n",
    "            X = np.column_stack([np.ones(len(prices2)), prices2])\n",
    "            coeffs = np.linalg.lstsq(X, prices1, rcond=None)[0]\n",
    "            alpha, beta = coeffs[0], coeffs[1]\n",
    "            residuals = prices1 - (alpha + beta * prices2)\n",
    "\n",
    "            spreads['residual'] = {\n",
    "                'values': residuals,\n",
    "                'description': 'OLS Residual Spread',\n",
    "                'formula': f'P_{self.symbol1} - (α + β·P_{self.symbol2})',\n",
    "                'beta': beta,\n",
    "                'alpha': alpha\n",
    "            }\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return spreads\n",
    "    \n",
    "    def augmented_dickey_fuller_test(self, spread_values, spread_name, significance_level=0.05):\n",
    "        \"\"\"Perform ADF test for stationarity.\"\"\"\n",
    "        try:\n",
    "            adf_result = adfuller(spread_values, autolag='AIC', regression='c')\n",
    "            adf_stat, p_value, lags, nobs, critical_values, ic_best = adf_result\n",
    "            \n",
    "            is_cointegrated = p_value < significance_level\n",
    "            \n",
    "            if adf_stat < critical_values['1%']:\n",
    "                significance = \"1% (very strong)\"\n",
    "            elif adf_stat < critical_values['5%']:\n",
    "                significance = \"5% (strong)\"\n",
    "            elif adf_stat < critical_values['10%']:\n",
    "                significance = \"10% (moderate)\"\n",
    "            else:\n",
    "                significance = \"None\"\n",
    "            \n",
    "            return {\n",
    "                'adf_statistic': adf_stat,\n",
    "                'p_value': p_value,\n",
    "                'lags_used': lags,\n",
    "                'n_observations': nobs,\n",
    "                'critical_values': critical_values,\n",
    "                'is_cointegrated': is_cointegrated,\n",
    "                'significance_achieved': significance\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"     ADF test failed for {spread_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def estimate_half_life(self, spread_values, weights=None):\n",
    "        \"\"\"Estimate mean reversion half-life.\"\"\"\n",
    "        try:\n",
    "            spread_changes = np.diff(spread_values)\n",
    "            spread_lagged = spread_values[:-1]\n",
    "            \n",
    "            valid_mask = ~(np.isnan(spread_changes) | np.isnan(spread_lagged))\n",
    "            if np.sum(valid_mask) < 20:\n",
    "                return None\n",
    "                \n",
    "            y = spread_changes[valid_mask]\n",
    "            X = np.column_stack([np.ones(np.sum(valid_mask)), spread_lagged[valid_mask]])\n",
    "            \n",
    "            if weights is not None:\n",
    "                w = weights[1:][valid_mask]\n",
    "                w = w / np.sum(w)\n",
    "                W = np.diag(w)\n",
    "                coeffs = np.linalg.lstsq(X.T @ W @ X, X.T @ W @ y, rcond=None)[0]\n",
    "            else:\n",
    "                coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "            \n",
    "            beta = coeffs[1]\n",
    "            \n",
    "            if -2 < beta < 0:\n",
    "                half_life = -np.log(2) / np.log(1 + beta)\n",
    "                return half_life if 0 < half_life < 1000 else None\n",
    "            \n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def detect_volume_regimes(self, volume_data, method='kmeans', n_regimes=3):\n",
    "        \"\"\"Detect volume regimes using clustering.\"\"\"\n",
    "        if method == 'kmeans':\n",
    "            volume_features = pd.DataFrame({\n",
    "                'volume_level': volume_data,\n",
    "                'volume_change': volume_data.pct_change(),\n",
    "                'volume_ma_ratio': volume_data / volume_data.rolling(20).mean(),\n",
    "            }).fillna(method='bfill')\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            features_scaled = scaler.fit_transform(volume_features)\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_regimes, random_state=42, n_init=10)\n",
    "            regimes = kmeans.fit_predict(features_scaled)\n",
    "            \n",
    "            cluster_means = volume_features.groupby(regimes)['volume_level'].mean()\n",
    "            cluster_mapping = {old: new for new, old in enumerate(cluster_means.argsort())}\n",
    "            regimes = pd.Series(regimes).map(cluster_mapping)\n",
    "            \n",
    "        elif method == 'percentile':\n",
    "            volume_percentiles = volume_data.rolling(100).rank(pct=True)\n",
    "            regimes = pd.cut(volume_percentiles, bins=[0, 0.33, 0.67, 1.0], \n",
    "                           labels=[0, 1, 2], include_lowest=True).astype(float)\n",
    "        \n",
    "        return regimes.fillna(method='ffill')\n",
    "    \n",
    "    def calculate_dominance_proxy(self, prices1, prices2, volume1, volume2):\n",
    "        \"\"\"Calculate relative dominance proxy.\"\"\"\n",
    "        weight1 = prices1 * volume1\n",
    "        weight2 = prices2 * volume2\n",
    "        total_weight = weight1 + weight2\n",
    "        \n",
    "        dominance = weight1 / total_weight\n",
    "        dominance_ma_short = pd.Series(dominance).rolling(10).mean()\n",
    "        dominance_ma_long = pd.Series(dominance).rolling(50).mean()\n",
    "        \n",
    "        return {\n",
    "            'dominance_proxy': dominance,\n",
    "            'dominance_ma_short': dominance_ma_short,\n",
    "            'dominance_ma_long': dominance_ma_long,\n",
    "            'dominance_trending_up': dominance_ma_short > dominance_ma_long,\n",
    "            'dominance_change': pd.Series(dominance).pct_change(),\n",
    "            'dominance_momentum': pd.Series(dominance).rolling(5).mean().pct_change()\n",
    "        }\n",
    "    \n",
    "    def volume_weighted_half_life(self, spread_values, volume_data, threshold=0.5):\n",
    "        \"\"\"Calculate volume-weighted half-lives.\"\"\"\n",
    "        volume_percentile = pd.Series(volume_data).rolling(50).rank(pct=True)\n",
    "        high_vol_mask = volume_percentile > threshold\n",
    "        low_vol_mask = volume_percentile <= threshold\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if np.sum(high_vol_mask) > 20:\n",
    "            results['high_volume_half_life'] = self.estimate_half_life(spread_values[high_vol_mask])\n",
    "        \n",
    "        if np.sum(low_vol_mask) > 20:\n",
    "            results['low_volume_half_life'] = self.estimate_half_life(spread_values[low_vol_mask])\n",
    "        \n",
    "        try:\n",
    "            volume_weights = volume_data / np.sum(volume_data)\n",
    "            results['volume_weighted_half_life'] = self.estimate_half_life(spread_values, weights=volume_weights)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_volume_enhanced_signals(self, spread_values, volume_regimes, dominance_metrics, \n",
    "                                        entry_threshold=2.0, exit_threshold=0.5):\n",
    "        \"\"\"Generate trading signals with volume context.\"\"\"\n",
    "        df = pd.DataFrame({\n",
    "            'spread': spread_values,\n",
    "            'volume_regime': volume_regimes,\n",
    "            'dominance_trending': dominance_metrics['dominance_trending_up'],\n",
    "            'dominance_momentum': dominance_metrics['dominance_momentum']\n",
    "        })\n",
    "        \n",
    "        df['spread_zscore'] = (df['spread'] - df['spread'].rolling(50).mean()) / df['spread'].rolling(50).std()\n",
    "        \n",
    "        df['base_long_signal'] = df['spread_zscore'] < -entry_threshold\n",
    "        df['base_short_signal'] = df['spread_zscore'] > entry_threshold\n",
    "        df['exit_signal'] = np.abs(df['spread_zscore']) < exit_threshold\n",
    "        \n",
    "        high_vol = df['volume_regime'] == 2\n",
    "        med_vol = df['volume_regime'] == 1\n",
    "        low_vol = df['volume_regime'] == 0\n",
    "        \n",
    "        strong_trend_1 = df['dominance_trending'] & (df['dominance_momentum'] > 0.01)\n",
    "        strong_trend_2 = ~df['dominance_trending'] & (df['dominance_momentum'] < -0.01)\n",
    "        \n",
    "        df['enhanced_long'] = df['base_long_signal'] & high_vol\n",
    "        df['enhanced_short'] = df['base_short_signal'] & high_vol\n",
    "        \n",
    "        df['enhanced_long'] |= df['base_long_signal'] & med_vol & ~strong_trend_1\n",
    "        df['enhanced_short'] |= df['base_short_signal'] & med_vol & ~strong_trend_2\n",
    "        \n",
    "        extreme_long = df['spread_zscore'] < -2.5\n",
    "        extreme_short = df['spread_zscore'] > 2.5\n",
    "        df['enhanced_long'] |= extreme_long & low_vol & ~strong_trend_1\n",
    "        df['enhanced_short'] |= extreme_short & low_vol & ~strong_trend_2\n",
    "        \n",
    "        df['signal_strength'] = 0.0\n",
    "        df.loc[df['enhanced_long'] | df['enhanced_short'], 'signal_strength'] = 1.0\n",
    "        df.loc[high_vol & (df['enhanced_long'] | df['enhanced_short']), 'signal_strength'] = 2.0\n",
    "        df.loc[med_vol & (df['enhanced_long'] | df['enhanced_short']), 'signal_strength'] = 1.5\n",
    "        df.loc[low_vol & (df['enhanced_long'] | df['enhanced_short']), 'signal_strength'] = 0.8\n",
    "        \n",
    "        return df[['enhanced_long', 'enhanced_short', 'exit_signal', 'signal_strength', \n",
    "                  'spread_zscore', 'volume_regime']]\n",
    "    \n",
    "    def calculate_correlation_metrics(self, prices1, prices2, window=50):\n",
    "        \"\"\"Calculate rolling correlation and other relationship metrics.\"\"\"\n",
    "        returns1 = pd.Series(prices1).pct_change()\n",
    "        returns2 = pd.Series(prices2).pct_change()\n",
    "        \n",
    "        rolling_corr = returns1.rolling(window).corr(returns2)\n",
    "        \n",
    "        return {\n",
    "            'static_correlation': np.corrcoef(returns1.dropna(), returns2.dropna())[0, 1],\n",
    "            'rolling_correlation': rolling_corr,\n",
    "            'mean_rolling_corr': rolling_corr.mean(),\n",
    "            'price_correlation': np.corrcoef(prices1, prices2)[0, 1],\n",
    "            'returns1': returns1,\n",
    "            'returns2': returns2\n",
    "        }\n",
    "    \n",
    "    def diagnose_non_cointegration(self, result):\n",
    "        \"\"\"Provide detailed diagnostics when cointegration fails.\"\"\"\n",
    "        diagnostics = {}\n",
    "        \n",
    "        # Check correlation\n",
    "        corr_metrics = self.calculate_correlation_metrics(\n",
    "            result['prices1'], result['prices2']\n",
    "        )\n",
    "        diagnostics['correlation'] = corr_metrics\n",
    "        \n",
    "        # Check for unit roots in individual series\n",
    "        try:\n",
    "            adf1 = adfuller(result['prices1'], autolag='AIC')\n",
    "            adf2 = adfuller(result['prices2'], autolag='AIC')\n",
    "            \n",
    "            diagnostics['stationarity'] = {\n",
    "                f'{self.symbol1}_stationary': adf1[1] < 0.05,\n",
    "                f'{self.symbol2}_stationary': adf2[1] < 0.05,\n",
    "                f'{self.symbol1}_adf_pvalue': adf1[1],\n",
    "                f'{self.symbol2}_adf_pvalue': adf2[1]\n",
    "            }\n",
    "        except:\n",
    "            diagnostics['stationarity'] = None\n",
    "        \n",
    "        # Analyze spread behavior\n",
    "        best_spread = None\n",
    "        min_pvalue = 1.0\n",
    "        for spread_name, spread_data in result['traditional_results'].items():\n",
    "            if spread_data['p_value'] < min_pvalue:\n",
    "                min_pvalue = spread_data['p_value']\n",
    "                best_spread = spread_name\n",
    "        \n",
    "        if best_spread:\n",
    "            spread_values = result['traditional_results'][best_spread]['values']\n",
    "            diagnostics['best_spread'] = best_spread\n",
    "            diagnostics['best_pvalue'] = min_pvalue\n",
    "            diagnostics['spread_volatility'] = np.std(spread_values)\n",
    "            diagnostics['spread_range'] = np.ptp(spread_values)\n",
    "            \n",
    "            # Check for trending behavior\n",
    "            spread_returns = np.diff(spread_values)\n",
    "            diagnostics['spread_autocorr'] = pd.Series(spread_values).autocorr(lag=1)\n",
    "        \n",
    "        return diagnostics\n",
    "    \n",
    "    def analyze_timeframe(self, timeframe_label, min_observations=100):\n",
    "        \"\"\"Comprehensive volume-enhanced analysis for a timeframe.\"\"\"\n",
    "        print(f\"\\n   Analyzing: {timeframe_label}\")\n",
    "        \n",
    "        data1 = self.timeframe_data[timeframe_label][self.symbol1]\n",
    "        data2 = self.timeframe_data[timeframe_label][self.symbol2]\n",
    "        \n",
    "        aligned = data1.join(data2, on=\"datetime\", how=\"inner\", suffix=\"_2\")\n",
    "        \n",
    "        if aligned.height < min_observations:\n",
    "            return {'error': 'Insufficient data', 'timeframe': timeframe_label}\n",
    "        \n",
    "        clean_aligned = aligned.filter(\n",
    "            pl.col(\"close\").is_not_null() &\n",
    "            pl.col(\"close_2\").is_not_null() &\n",
    "            pl.col(\"volume\").is_not_null() &\n",
    "            pl.col(\"volume_2\").is_not_null() &\n",
    "            (pl.col(\"close\") > 0) &\n",
    "            (pl.col(\"close_2\") > 0) &\n",
    "            (pl.col(\"volume\") >= 0) &\n",
    "            (pl.col(\"volume_2\") >= 0) &\n",
    "            pl.col(\"close\").is_finite() &\n",
    "            pl.col(\"close_2\").is_finite() &\n",
    "            pl.col(\"volume\").is_finite() &\n",
    "            pl.col(\"volume_2\").is_finite()\n",
    "        )\n",
    "        \n",
    "        if clean_aligned.height < min_observations:\n",
    "            return {'error': 'Insufficient clean data', 'timeframe': timeframe_label}\n",
    "        \n",
    "        timestamps = clean_aligned['datetime'].to_numpy()\n",
    "        prices1 = clean_aligned['close'].to_numpy()\n",
    "        prices2 = clean_aligned['close_2'].to_numpy()\n",
    "        volume1 = clean_aligned['volume'].to_numpy()\n",
    "        volume2 = clean_aligned['volume_2'].to_numpy()\n",
    "        \n",
    "        print(f\"     Aligned data: {len(prices1):,} observations\")\n",
    "        \n",
    "        # Volume metrics\n",
    "        df_for_volume = clean_aligned.to_pandas().set_index('datetime')\n",
    "        total_volume = df_for_volume['volume'] + df_for_volume['volume_2']\n",
    "        \n",
    "        window = min(20, len(total_volume) // 10)\n",
    "        volume_metrics = {\n",
    "            'total_volume': total_volume,\n",
    "            'volume_share_1': df_for_volume['volume'] / total_volume,\n",
    "            'volume_ma': total_volume.rolling(window).mean(),\n",
    "            'volume_std': total_volume.rolling(window).std(),\n",
    "        }\n",
    "        \n",
    "        volume_metrics['volume_zscore'] = (\n",
    "            (total_volume - volume_metrics['volume_ma']) / volume_metrics['volume_std']\n",
    "        ).fillna(0)\n",
    "        \n",
    "        volume_metrics['volume_percentile'] = total_volume.rolling(\n",
    "            min(window*2, len(total_volume))\n",
    "        ).rank(pct=True).fillna(0.5)\n",
    "        \n",
    "        volume_metrics['high_volume_regime'] = volume_metrics['volume_percentile'] > 0.7\n",
    "        volume_metrics['low_volume_regime'] = volume_metrics['volume_percentile'] < 0.3\n",
    "        \n",
    "        # Dominance proxy\n",
    "        dominance_metrics = self.calculate_dominance_proxy(prices1, prices2, volume1, volume2)\n",
    "        \n",
    "        # Volume regimes\n",
    "        volume_regimes = self.detect_volume_regimes(total_volume)\n",
    "        \n",
    "        # Ensure consistent lengths\n",
    "        min_len = min(len(volume_regimes), len(prices1))\n",
    "        volume_regimes = volume_regimes[:min_len]\n",
    "        prices1 = prices1[:min_len]\n",
    "        prices2 = prices2[:min_len]\n",
    "        volume1 = volume1[:min_len]\n",
    "        volume2 = volume2[:min_len]\n",
    "        timestamps = timestamps[:min_len]\n",
    "        \n",
    "        regime_distribution = volume_regimes.value_counts().sort_index()\n",
    "        print(f\"     Volume regimes: Low={regime_distribution.get(0,0)}, \" +\n",
    "              f\"Med={regime_distribution.get(1,0)}, High={regime_distribution.get(2,0)}\")\n",
    "        \n",
    "        # Traditional cointegration\n",
    "        spreads = self.calculate_spread_formulations(prices1, prices2)\n",
    "        traditional_results = {}\n",
    "        \n",
    "        for spread_name, spread_info in spreads.items():\n",
    "            adf_result = self.augmented_dickey_fuller_test(spread_info['values'], spread_name)\n",
    "            if adf_result:\n",
    "                half_life = self.estimate_half_life(spread_info['values'])\n",
    "                traditional_results[spread_name] = {\n",
    "                    **spread_info, \n",
    "                    **adf_result,\n",
    "                    'half_life': half_life\n",
    "                }\n",
    "        \n",
    "        # Enhanced half-lives\n",
    "        enhanced_half_lives = {}\n",
    "        for spread_name, spread_data in traditional_results.items():\n",
    "            if spread_data.get('is_cointegrated', False):\n",
    "                vol_hl = self.volume_weighted_half_life(\n",
    "                    spread_data['values'], \n",
    "                    volume_metrics['total_volume']\n",
    "                )\n",
    "                enhanced_half_lives[spread_name] = vol_hl\n",
    "        \n",
    "        # Generate signals\n",
    "        signals_df = None\n",
    "        if traditional_results:\n",
    "            best_spread = None\n",
    "            for s in traditional_results:\n",
    "                if traditional_results[s].get('is_cointegrated', False):\n",
    "                    if best_spread is None or traditional_results[s]['p_value'] < traditional_results[best_spread]['p_value']:\n",
    "                        best_spread = s\n",
    "            \n",
    "            if best_spread:\n",
    "                signals_df = self.generate_volume_enhanced_signals(\n",
    "                    traditional_results[best_spread]['values'],\n",
    "                    volume_regimes,\n",
    "                    dominance_metrics\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            'timeframe': timeframe_label,\n",
    "            'n_observations': len(prices1),\n",
    "            'timestamps': timestamps,\n",
    "            'prices1': prices1,\n",
    "            'prices2': prices2,\n",
    "            'volume1': volume1,\n",
    "            'volume2': volume2,\n",
    "            'volume_metrics': volume_metrics,\n",
    "            'dominance_metrics': dominance_metrics,\n",
    "            'volume_regimes': volume_regimes,\n",
    "            'traditional_results': traditional_results,\n",
    "            'enhanced_half_lives': enhanced_half_lives,\n",
    "            'signals': signals_df,\n",
    "            'regime_distribution': regime_distribution.to_dict()\n",
    "        }\n",
    "    \n",
    "    def run_analysis(self, timeframes_to_analyze=None):\n",
    "        \"\"\"Run full analysis across all timeframes.\"\"\"\n",
    "        if timeframes_to_analyze is None:\n",
    "            timeframes_to_analyze = list(self.timeframe_data.keys())\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"VOLUME-ENHANCED COINTEGRATION ANALYSIS\")\n",
    "        print(f\"{self.symbol1} vs {self.symbol2}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for tf_label in timeframes_to_analyze:\n",
    "            if tf_label in self.timeframe_data:\n",
    "                result = self.analyze_timeframe(tf_label)\n",
    "                self.results[tf_label] = result\n",
    "                \n",
    "                if 'error' not in result:\n",
    "                    coint_count = sum(1 for s in result['traditional_results'].values() \n",
    "                                    if s.get('is_cointegrated', False))\n",
    "                    total_spreads = len(result['traditional_results'])\n",
    "                    \n",
    "                    print(f\"\\n{tf_label}: Cointegration: {coint_count}/{total_spreads} spreads\", end=\"\")\n",
    "                    \n",
    "                    # Show closest p-value if no cointegration\n",
    "                    if coint_count == 0:\n",
    "                        min_pval = min(s['p_value'] for s in result['traditional_results'].values())\n",
    "                        print(f\" (best p={min_pval:.3f})\")\n",
    "                    else:\n",
    "                        print()\n",
    "                    \n",
    "                    regime_dist = result['regime_distribution']\n",
    "                    total_obs = sum(regime_dist.values())\n",
    "                    print(f\"       Volume regimes: Low={regime_dist.get(0,0)/total_obs:.1%}, \" +\n",
    "                          f\"Med={regime_dist.get(1,0)/total_obs:.1%}, High={regime_dist.get(2,0)/total_obs:.1%}\")\n",
    "                    \n",
    "                    # Add correlation info\n",
    "                    corr_metrics = self.calculate_correlation_metrics(result['prices1'], result['prices2'])\n",
    "                    print(f\"       Price correlation: {corr_metrics['price_correlation']:.3f}, \" +\n",
    "                          f\"Returns correlation: {corr_metrics['static_correlation']:.3f}\")\n",
    "                    \n",
    "                    if result['enhanced_half_lives']:\n",
    "                        for spread_name, hl_metrics in result['enhanced_half_lives'].items():\n",
    "                            if 'high_volume_half_life' in hl_metrics and 'low_volume_half_life' in hl_metrics:\n",
    "                                print(f\"       {spread_name}: HL_high={hl_metrics['high_volume_half_life']:.1f}, \" +\n",
    "                                      f\"HL_low={hl_metrics['low_volume_half_life']:.1f}\")\n",
    "                    \n",
    "                    # Store diagnostics\n",
    "                    result['diagnostics'] = self.diagnose_non_cointegration(result)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive trading recommendations.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TRADING RECOMMENDATIONS: {self.symbol1} vs {self.symbol2}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        viable_strategies = []\n",
    "        has_cointegration = False\n",
    "        \n",
    "        for tf_label, result in self.results.items():\n",
    "            if 'error' in result:\n",
    "                continue\n",
    "            \n",
    "            cointegrated_spreads = [s for s in result['traditional_results'] \n",
    "                                  if result['traditional_results'][s].get('is_cointegrated', False)]\n",
    "            \n",
    "            if cointegrated_spreads:\n",
    "                has_cointegration = True\n",
    "                print(f\"\\n{tf_label.upper()} TIMEFRAME:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                best_spread = min(cointegrated_spreads, \n",
    "                                key=lambda x: result['traditional_results'][x]['p_value'])\n",
    "                spread_data = result['traditional_results'][best_spread]\n",
    "                \n",
    "                print(f\"VIABLE: {best_spread.upper()} spread shows cointegration\")\n",
    "                print(f\"  p-value: {spread_data['p_value']:.4f}\")\n",
    "                \n",
    "                traditional_hl = spread_data.get('half_life')\n",
    "                if traditional_hl:\n",
    "                    print(f\"  Half-life: {traditional_hl:.1f} periods\")\n",
    "                \n",
    "                enhanced_hl = result['enhanced_half_lives'].get(best_spread, {})\n",
    "                if enhanced_hl:\n",
    "                    high_vol_hl = enhanced_hl.get('high_volume_half_life')\n",
    "                    low_vol_hl = enhanced_hl.get('low_volume_half_life')\n",
    "                    \n",
    "                    if high_vol_hl and low_vol_hl:\n",
    "                        print(f\"  High vol HL: {high_vol_hl:.1f} | Low vol HL: {low_vol_hl:.1f}\")\n",
    "                        speed_ratio = low_vol_hl / high_vol_hl\n",
    "                        print(f\"  -> {speed_ratio:.1f}x faster reversion in high volume\")\n",
    "                \n",
    "                # Calculate viability score\n",
    "                score = 0\n",
    "                if spread_data['p_value'] < 0.01:\n",
    "                    score += 30\n",
    "                elif spread_data['p_value'] < 0.05:\n",
    "                    score += 20\n",
    "                \n",
    "                if traditional_hl and traditional_hl < 20:\n",
    "                    score += 30\n",
    "                elif traditional_hl and traditional_hl < 50:\n",
    "                    score += 20\n",
    "                \n",
    "                if enhanced_hl and enhanced_hl.get('high_volume_half_life') and traditional_hl:\n",
    "                    if enhanced_hl['high_volume_half_life'] < traditional_hl * 0.5:\n",
    "                        score += 20\n",
    "                \n",
    "                if result['signals'] is not None:\n",
    "                    total_signals = np.sum(result['signals']['enhanced_long']) + np.sum(result['signals']['enhanced_short'])\n",
    "                    if total_signals > 0:\n",
    "                        strong_signals = np.sum(result['signals']['signal_strength'] >= 2.0)\n",
    "                        score += int(20 * strong_signals / total_signals)\n",
    "                \n",
    "                recommendation = \"STRONG\" if score >= 70 else \"MODERATE\" if score >= 50 else \"WEAK\"\n",
    "                print(f\"\\n  SCORE: {score}/100 - {recommendation}\")\n",
    "                \n",
    "                viable_strategies.append({\n",
    "                    'timeframe': tf_label,\n",
    "                    'spread': best_spread,\n",
    "                    'score': score,\n",
    "                    'recommendation': recommendation\n",
    "                })\n",
    "        \n",
    "        if not has_cointegration:\n",
    "            print(f\"\\nNO COINTEGRATION DETECTED\")\n",
    "            print(f\"\\nDIAGNOSTIC SUMMARY:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Analyze why cointegration failed\n",
    "            for tf_label, result in self.results.items():\n",
    "                if 'error' in result or 'diagnostics' not in result:\n",
    "                    continue\n",
    "                \n",
    "                diag = result['diagnostics']\n",
    "                print(f\"\\n{tf_label}:\")\n",
    "                \n",
    "                # Correlation analysis\n",
    "                if 'correlation' in diag:\n",
    "                    corr = diag['correlation']\n",
    "                    print(f\"  Price correlation: {corr['price_correlation']:.3f}\")\n",
    "                    print(f\"  Returns correlation: {corr['static_correlation']:.3f}\")\n",
    "                    \n",
    "                    if abs(corr['static_correlation']) < 0.3:\n",
    "                        print(f\"    -> Weak correlation suggests independent movements\")\n",
    "                    elif abs(corr['static_correlation']) > 0.7:\n",
    "                        print(f\"    -> Strong correlation but no cointegration\")\n",
    "                        print(f\"    -> Consider: Different growth rates or regime shifts\")\n",
    "                \n",
    "                # Best spread analysis\n",
    "                if 'best_spread' in diag:\n",
    "                    print(f\"  Closest to cointegration: {diag['best_spread']} (p={diag['best_pvalue']:.3f})\")\n",
    "                    \n",
    "                    if diag['best_pvalue'] < 0.10:\n",
    "                        print(f\"    -> Marginally close to cointegration threshold\")\n",
    "                    \n",
    "                    if 'spread_autocorr' in diag:\n",
    "                        print(f\"  Spread autocorrelation: {diag['spread_autocorr']:.3f}\")\n",
    "                        if diag['spread_autocorr'] > 0.95:\n",
    "                            print(f\"    -> High persistence suggests trending, not mean reversion\")\n",
    "                \n",
    "                # Stationarity check\n",
    "                if diag.get('stationarity'):\n",
    "                    stat = diag['stationarity']\n",
    "                    print(f\"  Stationarity: {self.symbol1}={stat[f'{self.symbol1}_stationary']}, \" +\n",
    "                          f\"{self.symbol2}={stat[f'{self.symbol2}_stationary']}\")\n",
    "                    \n",
    "                    if not stat[f'{self.symbol1}_stationary'] or not stat[f'{self.symbol2}_stationary']:\n",
    "                        print(f\"    -> Both series should be non-stationary for cointegration\")\n",
    "            \n",
    "            print(f\"\\nRECOMMENDATIONS:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"• These assets do not show mean-reverting relationship\")\n",
    "            print(f\"• Consider alternative strategies:\")\n",
    "            print(f\"  - Momentum/trend following\")\n",
    "            print(f\"  - Correlation trading (if correlation is high)\")\n",
    "            print(f\"  - Individual asset analysis\")\n",
    "            print(f\"• Or try different cryptocurrency pairs\")\n",
    "        \n",
    "        if viable_strategies:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"RANKED STRATEGIES:\")\n",
    "            viable_strategies.sort(key=lambda x: x['score'], reverse=True)\n",
    "            for i, strat in enumerate(viable_strategies, 1):\n",
    "                print(f\"{i}. {strat['timeframe']} - {strat['spread'].upper()}: {strat['score']}/100 ({strat['recommendation']})\")\n",
    "    \n",
    "    def plot_price_comparison(self, timeframe='1D', figsize=(15, 10)):\n",
    "        \"\"\"Plot comprehensive price comparison analysis.\"\"\"\n",
    "        if timeframe not in self.results or 'error' in self.results[timeframe]:\n",
    "            print(f\"No valid results for {timeframe}\")\n",
    "            return\n",
    "        \n",
    "        result = self.results[timeframe]\n",
    "        timestamps = pd.to_datetime(result['timestamps'])\n",
    "        \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = fig.add_gridspec(4, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Plot 1: Price series with dual axes\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax1_twin = ax1.twinx()\n",
    "        \n",
    "        line1 = ax1.plot(timestamps, result['prices1'], \n",
    "                        label=self.symbol1, color='#f7931a', linewidth=2, alpha=0.8)\n",
    "        line2 = ax1_twin.plot(timestamps, result['prices2'], \n",
    "                             label=self.symbol2, color='#627eea', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax1.set_ylabel(f'{self.symbol1} Price', color='#f7931a', fontweight='bold')\n",
    "        ax1_twin.set_ylabel(f'{self.symbol2} Price', color='#627eea', fontweight='bold')\n",
    "        ax1.set_xlabel('Date', fontweight='bold')\n",
    "        ax1.set_title(f'{timeframe} - Price Evolution: {self.symbol1} vs {self.symbol2}', \n",
    "                     fontweight='bold', fontsize=14)\n",
    "        \n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax1.legend(lines, labels, loc='upper left', fontsize=11)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Normalized prices\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        norm_prices1 = 100 * result['prices1'] / result['prices1'][0]\n",
    "        norm_prices2 = 100 * result['prices2'] / result['prices2'][0]\n",
    "        \n",
    "        ax2.plot(timestamps, norm_prices1, label=self.symbol1, \n",
    "                color='#f7931a', linewidth=2, alpha=0.8)\n",
    "        ax2.plot(timestamps, norm_prices2, label=self.symbol2, \n",
    "                color='#627eea', linewidth=2, alpha=0.8)\n",
    "        ax2.axhline(100, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        ax2.set_ylabel('Normalized Price (Base=100)', fontweight='bold')\n",
    "        ax2.set_xlabel('Date', fontweight='bold')\n",
    "        ax2.set_title('Relative Performance', fontweight='bold')\n",
    "        ax2.legend(fontsize=10)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Returns correlation\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        if 'diagnostics' in result and 'correlation' in result['diagnostics']:\n",
    "            corr_data = result['diagnostics']['correlation']\n",
    "            returns1 = corr_data['returns1']\n",
    "            returns2 = corr_data['returns2']\n",
    "            \n",
    "            valid_mask = ~(np.isnan(returns1) | np.isnan(returns2))\n",
    "            ret1_valid = returns1[valid_mask]\n",
    "            ret2_valid = returns2[valid_mask]\n",
    "            \n",
    "            ax3.scatter(ret1_valid, ret2_valid, alpha=0.3, s=10, color='purple')\n",
    "            \n",
    "            z = np.polyfit(ret1_valid, ret2_valid, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_line = np.linspace(ret1_valid.min(), ret1_valid.max(), 100)\n",
    "            ax3.plot(x_line, p(x_line), \"r--\", linewidth=2, alpha=0.8, \n",
    "                    label=f'Corr={corr_data[\"static_correlation\"]:.3f}')\n",
    "            \n",
    "            ax3.set_xlabel(f'{self.symbol1} Returns', fontweight='bold')\n",
    "            ax3.set_ylabel(f'{self.symbol2} Returns', fontweight='bold')\n",
    "            ax3.set_title('Returns Correlation', fontweight='bold')\n",
    "            ax3.legend(fontsize=10)\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            ax3.axhline(0, color='black', linestyle='-', alpha=0.3, linewidth=0.5)\n",
    "            ax3.axvline(0, color='black', linestyle='-', alpha=0.3, linewidth=0.5)\n",
    "        \n",
    "        # Plot 4: Best spread behavior\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        if result['traditional_results']:\n",
    "            best_spread = min(result['traditional_results'].items(), \n",
    "                            key=lambda x: x[1]['p_value'])\n",
    "            spread_name, spread_data = best_spread\n",
    "            spread_values = spread_data['values']\n",
    "            \n",
    "            ax4.plot(timestamps, spread_values, color='darkgreen', linewidth=1.5, alpha=0.8)\n",
    "            ax4.axhline(np.mean(spread_values), color='red', linestyle='--', \n",
    "                       linewidth=2, alpha=0.8, label='Mean')\n",
    "            \n",
    "            spread_mean = np.mean(spread_values)\n",
    "            spread_std = np.std(spread_values)\n",
    "            ax4.axhline(spread_mean + spread_std, color='orange', linestyle=':', alpha=0.6)\n",
    "            ax4.axhline(spread_mean - spread_std, color='orange', linestyle=':', alpha=0.6)\n",
    "            ax4.fill_between(timestamps, spread_mean - spread_std, spread_mean + spread_std,\n",
    "                           alpha=0.2, color='yellow')\n",
    "            \n",
    "            is_coint = spread_data.get('is_cointegrated', False)\n",
    "            coint_status = \"COINTEGRATED\" if is_coint else \"NOT COINTEGRATED\"\n",
    "            ax4.set_title(f'{spread_name.upper()} Spread - p={spread_data[\"p_value\"]:.4f} {coint_status}', \n",
    "                         fontweight='bold', color='green' if is_coint else 'red')\n",
    "            ax4.set_ylabel('Spread Value', fontweight='bold')\n",
    "            ax4.set_xlabel('Date', fontweight='bold')\n",
    "            ax4.legend(fontsize=10)\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Rolling correlation\n",
    "        ax5 = fig.add_subplot(gs[3, 0])\n",
    "        if 'diagnostics' in result and 'correlation' in result['diagnostics']:\n",
    "            rolling_corr = result['diagnostics']['correlation']['rolling_correlation']\n",
    "            ax5.plot(timestamps, rolling_corr, color='purple', linewidth=1.5)\n",
    "            ax5.axhline(0, color='black', linestyle='-', alpha=0.3, linewidth=1)\n",
    "            ax5.axhline(rolling_corr.mean(), color='red', linestyle='--', \n",
    "                       linewidth=2, alpha=0.8, label=f'Mean={rolling_corr.mean():.3f}')\n",
    "            ax5.fill_between(timestamps, 0, rolling_corr, \n",
    "                           where=(rolling_corr > 0), alpha=0.3, color='green', \n",
    "                           interpolate=True)\n",
    "            ax5.fill_between(timestamps, 0, rolling_corr, \n",
    "                           where=(rolling_corr < 0), alpha=0.3, color='red', \n",
    "                           interpolate=True)\n",
    "            \n",
    "            ax5.set_ylabel('Rolling Correlation (50-period)', fontweight='bold')\n",
    "            ax5.set_xlabel('Date', fontweight='bold')\n",
    "            ax5.set_title('Time-Varying Correlation', fontweight='bold')\n",
    "            ax5.set_ylim(-1, 1)\n",
    "            ax5.legend(fontsize=9)\n",
    "            ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Volume analysis\n",
    "        ax6 = fig.add_subplot(gs[3, 1])\n",
    "        total_volume = result['volume_metrics']['total_volume']\n",
    "        volume_regimes = result['volume_regimes']\n",
    "        \n",
    "        regime_colors = {0: '#ff6b6b', 1: '#4ecdc4', 2: '#45b7d1'}\n",
    "        for regime in [0, 1, 2]:\n",
    "            mask = volume_regimes == regime\n",
    "            if np.any(mask):\n",
    "                ax6.fill_between(timestamps, 0, total_volume.max() * 1.1,\n",
    "                               where=mask, alpha=0.3, color=regime_colors[regime])\n",
    "        \n",
    "        ax6.fill_between(timestamps, 0, total_volume, alpha=0.6, color='purple')\n",
    "        ax6.plot(timestamps, result['volume_metrics']['volume_ma'], \n",
    "                color='darkviolet', linewidth=2, label='Volume MA')\n",
    "        \n",
    "        ax6.set_ylabel('Total Volume', fontweight='bold')\n",
    "        ax6.set_xlabel('Date', fontweight='bold')\n",
    "        ax6.set_title('Volume with Regimes', fontweight='bold')\n",
    "        ax6.legend(fontsize=9)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'{self.symbol1} vs {self.symbol2} - Comprehensive Analysis ({timeframe})', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_spread_analysis(self, timeframe='1D', figsize=(15, 10)):\n",
    "        \"\"\"Detailed spread analysis visualization.\"\"\"\n",
    "        if timeframe not in self.results or 'error' in self.results[timeframe]:\n",
    "            print(f\"No valid results for {timeframe}\")\n",
    "            return\n",
    "        \n",
    "        result = self.results[timeframe]\n",
    "        timestamps = pd.to_datetime(result['timestamps'])\n",
    "        \n",
    "        spreads_to_plot = list(result['traditional_results'].keys())\n",
    "        n_spreads = len(spreads_to_plot)\n",
    "        \n",
    "        if n_spreads == 0:\n",
    "            print(\"No spreads to analyze\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(n_spreads, 2, figsize=(figsize[0], 4*n_spreads))\n",
    "        if n_spreads == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for idx, spread_name in enumerate(spreads_to_plot):\n",
    "            spread_data = result['traditional_results'][spread_name]\n",
    "            spread_values = spread_data['values']\n",
    "            \n",
    "            # Left: Spread time series\n",
    "            ax_left = axes[idx, 0]\n",
    "            ax_left.plot(timestamps, spread_values, color='darkblue', linewidth=1.5, alpha=0.8)\n",
    "            \n",
    "            spread_mean = np.mean(spread_values)\n",
    "            spread_std = np.std(spread_values)\n",
    "            ax_left.axhline(spread_mean, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "            ax_left.axhline(spread_mean + 2*spread_std, color='orange', linestyle=':', linewidth=1.5)\n",
    "            ax_left.axhline(spread_mean - 2*spread_std, color='orange', linestyle=':', linewidth=1.5)\n",
    "            ax_left.fill_between(timestamps, spread_mean - 2*spread_std, spread_mean + 2*spread_std,\n",
    "                               alpha=0.2, color='yellow', label='±2σ')\n",
    "            \n",
    "            is_coint = spread_data.get('is_cointegrated', False)\n",
    "            color = 'green' if is_coint else 'red'\n",
    "            status = \"COINTEGRATED\" if is_coint else \"NOT COINTEGRATED\"\n",
    "            \n",
    "            ax_left.set_title(f'{spread_name.upper()} Spread - {status}\\nADF p-value: {spread_data[\"p_value\"]:.4f}', \n",
    "                            fontweight='bold', color=color, fontsize=11)\n",
    "            ax_left.set_ylabel('Spread Value', fontweight='bold')\n",
    "            ax_left.set_xlabel('Date', fontweight='bold')\n",
    "            ax_left.legend(fontsize=9)\n",
    "            ax_left.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Right: Distribution\n",
    "            ax_right = axes[idx, 1]\n",
    "            ax_right.hist(spread_values, bins=50, alpha=0.6, color='steelblue', \n",
    "                         edgecolor='black', density=True)\n",
    "            \n",
    "            from scipy.stats import norm\n",
    "            mu, sigma = norm.fit(spread_values)\n",
    "            x = np.linspace(spread_values.min(), spread_values.max(), 100)\n",
    "            ax_right.plot(x, norm.pdf(x, mu, sigma), 'r-', linewidth=2, \n",
    "                         label=f'Normal fit\\nμ={mu:.3f}, σ={sigma:.3f}')\n",
    "            \n",
    "            ax_right.axvline(spread_mean, color='red', linestyle='--', linewidth=2)\n",
    "            ax_right.set_title(f'Distribution Analysis', fontweight='bold', fontsize=11)\n",
    "            ax_right.set_xlabel('Spread Value', fontweight='bold')\n",
    "            ax_right.set_ylabel('Density', fontweight='bold')\n",
    "            ax_right.legend(fontsize=9)\n",
    "            ax_right.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            stats_text = f'Skewness: {pd.Series(spread_values).skew():.3f}\\n'\n",
    "            stats_text += f'Kurtosis: {pd.Series(spread_values).kurtosis():.3f}\\n'\n",
    "            if 'half_life' in spread_data and spread_data['half_life']:\n",
    "                stats_text += f'Half-life: {spread_data[\"half_life\"]:.1f}'\n",
    "            \n",
    "            ax_right.text(0.02, 0.98, stats_text, transform=ax_right.transAxes,\n",
    "                        fontsize=9, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.suptitle(f'{self.symbol1} vs {self.symbol2} - Spread Analysis ({timeframe})', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_cointegration_heatmap(self, figsize=(12, 8)):\n",
    "        \"\"\"Create heatmap of cointegration test results.\"\"\"\n",
    "        timeframes = list(self.results.keys())\n",
    "        spread_types = []\n",
    "        \n",
    "        for result in self.results.values():\n",
    "            if 'error' not in result and result['traditional_results']:\n",
    "                spread_types = list(result['traditional_results'].keys())\n",
    "                break\n",
    "        \n",
    "        if not spread_types:\n",
    "            print(\"No spread data available\")\n",
    "            return\n",
    "        \n",
    "        pvalue_matrix = np.full((len(timeframes), len(spread_types)), np.nan)\n",
    "        coint_matrix = np.full((len(timeframes), len(spread_types)), 0)\n",
    "        \n",
    "        for i, tf in enumerate(timeframes):\n",
    "            result = self.results[tf]\n",
    "            if 'error' not in result and result['traditional_results']:\n",
    "                for j, spread in enumerate(spread_types):\n",
    "                    if spread in result['traditional_results']:\n",
    "                        pvalue_matrix[i, j] = result['traditional_results'][spread]['p_value']\n",
    "                        coint_matrix[i, j] = 1 if result['traditional_results'][spread].get('is_cointegrated', False) else 0\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Heatmap 1: P-values\n",
    "        im1 = ax1.imshow(pvalue_matrix, cmap='RdYlGn_r', aspect='auto', vmin=0, vmax=0.15)\n",
    "        ax1.set_xticks(range(len(spread_types)))\n",
    "        ax1.set_xticklabels([s.upper() for s in spread_types], rotation=45, ha='right')\n",
    "        ax1.set_yticks(range(len(timeframes)))\n",
    "        ax1.set_yticklabels(timeframes)\n",
    "        ax1.set_title('ADF Test P-Values\\n(Green=Low p-value)', fontweight='bold')\n",
    "        \n",
    "        for i in range(len(timeframes)):\n",
    "            for j in range(len(spread_types)):\n",
    "                if not np.isnan(pvalue_matrix[i, j]):\n",
    "                    text_color = 'white' if pvalue_matrix[i, j] < 0.05 else 'black'\n",
    "                    ax1.text(j, i, f'{pvalue_matrix[i, j]:.3f}',\n",
    "                           ha=\"center\", va=\"center\", color=text_color, fontweight='bold', fontsize=9)\n",
    "        \n",
    "        plt.colorbar(im1, ax=ax1, label='P-value')\n",
    "        \n",
    "        # Heatmap 2: Cointegration status\n",
    "        im2 = ax2.imshow(coint_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "        ax2.set_xticks(range(len(spread_types)))\n",
    "        ax2.set_xticklabels([s.upper() for s in spread_types], rotation=45, ha='right')\n",
    "        ax2.set_yticks(range(len(timeframes)))\n",
    "        ax2.set_yticklabels(timeframes)\n",
    "        ax2.set_title('Cointegration Status\\n(Green=Yes, Red=No)', fontweight='bold')\n",
    "        \n",
    "        for i in range(len(timeframes)):\n",
    "            for j in range(len(spread_types)):\n",
    "                status = 'YES' if coint_matrix[i, j] == 1 else 'NO'\n",
    "                text_color = 'white' if coint_matrix[i, j] == 1 else 'black'\n",
    "                ax2.text(j, i, status, ha=\"center\", va=\"center\", \n",
    "                       color=text_color, fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.colorbar(im2, ax=ax2, label='Cointegrated')\n",
    "        \n",
    "        plt.suptitle(f'{self.symbol1} vs {self.symbol2} - Cointegration Results', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_volume_regime_analysis(self, timeframe='1D', figsize=(15, 12)):\n",
    "        \"\"\"Comprehensive volume regime analysis.\"\"\"\n",
    "        if timeframe not in self.results or 'error' in self.results[timeframe]:\n",
    "            print(f\"No valid results for {timeframe}\")\n",
    "            return\n",
    "        \n",
    "        result = self.results[timeframe]\n",
    "        timestamps = pd.to_datetime(result['timestamps'])\n",
    "        volume_regimes = result['volume_regimes']\n",
    "        total_volume = result['volume_metrics']['total_volume']\n",
    "        dominance = result['dominance_metrics']['dominance_proxy']\n",
    "        \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = fig.add_gridspec(4, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Plot 1: Prices with volume regimes\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax1_twin = ax1.twinx()\n",
    "        \n",
    "        regime_colors = {0: 'lightcoral', 1: 'lightblue', 2: 'lightgreen'}\n",
    "        \n",
    "        for regime in [0, 1, 2]:\n",
    "            mask = volume_regimes == regime\n",
    "            if np.any(mask):\n",
    "                ax1.fill_between(timestamps, \n",
    "                               result['prices1'].min() * 0.95,\n",
    "                               result['prices1'].max() * 1.05,\n",
    "                               where=mask, alpha=0.3, color=regime_colors[regime])\n",
    "        \n",
    "        line1 = ax1.plot(timestamps, result['prices1'], label=self.symbol1, \n",
    "                        color='#f7931a', linewidth=2, alpha=0.8)\n",
    "        line2 = ax1_twin.plot(timestamps, result['prices2'], label=self.symbol2, \n",
    "                             color='#627eea', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax1.set_ylabel(f'{self.symbol1} Price', color='#f7931a', fontweight='bold')\n",
    "        ax1_twin.set_ylabel(f'{self.symbol2} Price', color='#627eea', fontweight='bold')\n",
    "        ax1.set_xlabel('Date', fontweight='bold')\n",
    "        ax1.set_title('Price Evolution with Volume Regimes', fontweight='bold', fontsize=13)\n",
    "        \n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax1.legend(lines, labels, loc='upper left', fontsize=10)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Volume regime distribution\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        regime_dist = result['regime_distribution']\n",
    "        colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\n",
    "        labels = ['Low\\nVolume', 'Medium\\nVolume', 'High\\nVolume']\n",
    "        \n",
    "        counts = [regime_dist.get(i, 0) for i in range(3)]\n",
    "        total = sum(counts)\n",
    "        percentages = [100 * c / total for c in counts]\n",
    "        \n",
    "        bars = ax2.bar(labels, counts, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "        ax2.set_ylabel('Number of Observations', fontweight='bold')\n",
    "        ax2.set_title('Volume Regime Distribution', fontweight='bold')\n",
    "        \n",
    "        for bar, pct, count in zip(bars, percentages, counts):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + total*0.01,\n",
    "                    f'{pct:.1f}%\\n({count:,})', ha='center', va='bottom', \n",
    "                    fontweight='bold', fontsize=10)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 3: Volume time series\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        ax3.fill_between(timestamps, 0, total_volume, alpha=0.5, color='purple')\n",
    "        ax3.plot(timestamps, result['volume_metrics']['volume_ma'], \n",
    "                color='darkviolet', linewidth=2.5, label='Volume MA')\n",
    "        \n",
    "        ax3.set_ylabel('Total Volume', fontweight='bold')\n",
    "        ax3.set_xlabel('Date', fontweight='bold')\n",
    "        ax3.set_title('Volume Activity Over Time', fontweight='bold')\n",
    "        ax3.legend(fontsize=9)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Dominance evolution\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        dom_ma_short = result['dominance_metrics']['dominance_ma_short']\n",
    "        dom_ma_long = result['dominance_metrics']['dominance_ma_long']\n",
    "        \n",
    "        ax4.plot(timestamps, dominance, color='gold', linewidth=1.5, alpha=0.6, label='Dominance Proxy')\n",
    "        ax4.plot(timestamps, dom_ma_short, color='orange', linewidth=2, label='Short MA (10)')\n",
    "        ax4.plot(timestamps, dom_ma_long, color='red', linewidth=2, label='Long MA (50)')\n",
    "        ax4.axhline(0.5, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        ax4.set_ylabel(f'{self.symbol1} Market Weight', fontweight='bold')\n",
    "        ax4.set_xlabel('Date', fontweight='bold')\n",
    "        ax4.set_title('Relative Dominance Trends', fontweight='bold')\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.legend(fontsize=9, loc='best')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Regime transitions\n",
    "        ax5 = fig.add_subplot(gs[3, 0])\n",
    "        transitions = np.diff(volume_regimes)\n",
    "        transition_types = {\n",
    "            'L→M': np.sum((transitions == 1)),\n",
    "            'M→H': np.sum((transitions == 1)),\n",
    "            'H→M': np.sum((transitions == -1)),\n",
    "            'M→L': np.sum((transitions == -1)),\n",
    "        }\n",
    "        \n",
    "        non_zero = {k: v for k, v in transition_types.items() if v > 0}\n",
    "        if non_zero:\n",
    "            bars = ax5.bar(range(len(non_zero)), list(non_zero.values()),\n",
    "                          color='skyblue', alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "            ax5.set_xticks(range(len(non_zero)))\n",
    "            ax5.set_xticklabels(list(non_zero.keys()))\n",
    "            ax5.set_ylabel('Transition Count', fontweight='bold')\n",
    "            ax5.set_title('Volume Regime Transitions', fontweight='bold')\n",
    "            ax5.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 6: Volume regime timeline\n",
    "        ax6 = fig.add_subplot(gs[3, 1])\n",
    "        for regime in [0, 1, 2]:\n",
    "            mask = volume_regimes == regime\n",
    "            if np.any(mask):\n",
    "                ax6.fill_between(timestamps, regime-0.4, regime+0.4,\n",
    "                               where=mask, alpha=0.7, color=regime_colors[regime])\n",
    "        \n",
    "        ax6.set_ylabel('Volume Regime', fontweight='bold')\n",
    "        ax6.set_xlabel('Date', fontweight='bold')\n",
    "        ax6.set_title('Regime Evolution Timeline', fontweight='bold')\n",
    "        ax6.set_yticks([0, 1, 2])\n",
    "        ax6.set_yticklabels(['Low', 'Medium', 'High'])\n",
    "        ax6.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.suptitle(f'{self.symbol1} vs {self.symbol2} - Volume Regime Analysis ({timeframe})', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        plt.show()\n",
    "    \n",
    "    def create_full_visualization_suite(self, timeframe='1D'):\n",
    "        \"\"\"Generate all visualizations.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"GENERATING VISUALIZATION SUITE: {self.symbol1} vs {self.symbol2}\")\n",
    "        print(f\"Timeframe: {timeframe}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        print(\"1. Price Comparison Analysis...\")\n",
    "        self.plot_price_comparison(timeframe=timeframe)\n",
    "        \n",
    "        print(\"\\n2. Spread Analysis...\")\n",
    "        self.plot_spread_analysis(timeframe=timeframe)\n",
    "        \n",
    "        print(\"\\n3. Cointegration Heatmap...\")\n",
    "        self.plot_cointegration_heatmap()\n",
    "        \n",
    "        print(\"\\n4. Volume Regime Analysis...\")\n",
    "        self.plot_volume_regime_analysis(timeframe=timeframe)\n",
    "        \n",
    "        print(\"\\nAll visualizations complete!\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer for any two crypto pairs\n",
    "    analyzer = CryptoCointegrationAnalyzer(\n",
    "        symbol1='BTCUSD',\n",
    "        symbol2='ETHUSD',\n",
    "        data_dir='coinbase/5m'\n",
    "    )\n",
    "    \n",
    "    # Load data\n",
    "    analyzer.load_data()\n",
    "    \n",
    "    # Create timeframe aggregations\n",
    "    TIMEFRAMES = {\n",
    "        '1H': '1h',\n",
    "        '4H': '4h', \n",
    "        '12H': '12h',\n",
    "        '1D': '1d',\n",
    "        '3D': '3d',\n",
    "    }\n",
    "    analyzer.create_timeframe_aggregations(TIMEFRAMES)\n",
    "    \n",
    "    # Run analysis\n",
    "    results = analyzer.run_analysis()\n",
    "    \n",
    "    # Generate report\n",
    "    analyzer.generate_report()\n",
    "    \n",
    "    # Create visualizations\n",
    "    analyzer.create_full_visualization_suite(timeframe='1D')\n",
    "    \n",
    "    # Access individual results\n",
    "    # results['1H']['traditional_results']\n",
    "    # results['4H']['signals']\n",
    "    \n",
    "    # Or analyze different pairs\n",
    "    # analyzer2 = CryptoCointegrationAnalyzer('SOLUSD', 'XRPUSD', 'coinbase/5m')\n",
    "    # analyzer2.load_data().create_timeframe_aggregations(TIMEFRAMES)\n",
    "    # analyzer2.run_analysis().generate_report()\n",
    "    # analyzer2.create_full_visualization_suite(timeframe='1D')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
